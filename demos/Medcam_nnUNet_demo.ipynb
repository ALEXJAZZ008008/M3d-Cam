{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Medcam_nnUNet_demo.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "source": [],
        "metadata": {
          "collapsed": false
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1h00Piy5ERmK",
        "colab_type": "text"
      },
      "source": [
        "# **Using M3d-CAM with the nnUNet**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OS3PKZ3YMy47",
        "colab_type": "text"
      },
      "source": [
        "In this demo you will learn how to use M3d-CAM with the nnUNet to extract 3D attention maps. We will use the [Medical Segmentation Decathlon](http://medicaldecathlon.com/) 3D prostate dataset as an example. The nnUNet splits the input data into patches and reconstructs them afterwards over the course of multiple classes. As a consequence the attention maps generated by M3d-CAM will also only be patches which need to be reconstructed afterwards. This will make the usage of M3d-CAM a little bit more complicated but not by much as you will see. \\\\\n",
        "\n",
        "This demonstration was made using Google Colab and probably won't work if you are not using Colab."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVSx7IY2EIFw",
        "colab_type": "text"
      },
      "source": [
        "# Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkl2JV-BNSzc",
        "colab_type": "text"
      },
      "source": [
        "Clone and install the nnUNet:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ClKFrxHNaAi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/MIC-DKFZ/nnUNet.git\n",
        "%cd nnUNet\n",
        "!git reset --hard b38c69b345b2f60cd0d053039669e8f988b0c0af # Reset repo to a specific commit as nnUNet code changes often. This ensures that the demo will work.\n",
        "!pip install -e ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qj7BnbeH_orr",
        "colab_type": "text"
      },
      "source": [
        "Install gdown to download files from google drive:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyaoz58m_u3h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install gdown"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJHguxcd_zx9",
        "colab_type": "text"
      },
      "source": [
        "Download the prostate dataset from google drive:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3VhL4UN_6-q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir -p /content/nnUNet_raw_data_base/nnUNet_raw_data\n",
        "!mkdir -p /content/nnUNet_raw_data_base/nnUNet_preprocessed\n",
        "!mkdir -p /content/nnUNet_trained_models\n",
        "%cd /content/nnUNet_raw_data_base/nnUNet_raw_data\n",
        "!gdown https://drive.google.com/uc?id=1Ff7c21UksxyT4JfETjaarmuKEjdqe1-a\n",
        "!tar -xvf Task05_Prostate.tar\n",
        "!rm Task05_Prostate.tar\n",
        "%cd .."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svpC5N96armV",
        "colab_type": "text"
      },
      "source": [
        "Set environment variables for the nnUNet:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlrMOpj1Mm8e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%env nnUNet_raw_data_base=/content/nnUNet_raw_data_base/nnUNet_raw_data\n",
        "%env nnUNet_preprocessed=/content/nnUNet_raw_data_base/nnUNet_preprocessed\n",
        "%env RESULTS_FOLDER=/content/nnUNet_trained_models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjD3A5G5a8wq",
        "colab_type": "text"
      },
      "source": [
        "Convert and preprocess the dataset (this might take some time):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X32Pml4uH5hB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nnUNet_convert_decathlon_task -i /content/nnUNet_raw_data_base/nnUNet_raw_data/Task05_Prostate -p 1\n",
        "!nnUNet_plan_and_preprocess -t 005 --verify_dataset_integrity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19aAeFNaayqU",
        "colab_type": "text"
      },
      "source": [
        "Download the pretrained model for the prostate dataset (this might take some time):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5MeFqguQL37R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nnUNet_download_pretrained_model Task005_Prostate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrzgksboVuk_",
        "colab_type": "text"
      },
      "source": [
        "Install M3d-CAM:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFW0O0n-VxQC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install medcam"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jf8HMvn6EiQs",
        "colab_type": "text"
      },
      "source": [
        "# Injecting M3d-CAM into nnUNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7EGy67IisvqQ",
        "colab_type": "text"
      },
      "source": [
        "Create a directory for the M3d-CAM evaluation results:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HY9XZyDJi7Sn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir /content/medcam_results"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCDQzyl_lTGf",
        "colab_type": "text"
      },
      "source": [
        "To inject nnUNet with M3d-CAM we need to modify the predict.py inside `nnUNet/nnunet/inference`. This is done with the following code which needs to be inserted into the file:\n",
        "\n",
        "```\n",
        "# Line 34\n",
        "from medcam import medcam\n",
        "# Line 188\n",
        "trainer.network = medcam.inject(trainer.network, label=1, replace=True, backend=\"gcam\", layer='seg_outputs.5')\n",
        "trainer.network.inference_apply_nonlin = lambda x: x\n",
        "```\n",
        "\n",
        "Herby the parameters of `medcam.inject` have the following meaning: \\\\\n",
        "`label=1`: nnUNet encodes the different classes it is able to predict/segment in the channel dimension. The label keyword tells medcam that we are only interested in the attention maps for the second class. \\\\\n",
        "`replace=True`: We also tell M3d-CAM that it should return the attention maps patches whenever the model forward() is called instead of the normal prediction/segmentation. As a result nnUNet will reconstruct our patches to full attention maps for us.  \\\\\n",
        "`backend=\"gcam\"`: The backend which we want to use. In this case Grad-CAM. \\\\\n",
        "`layer='seg_outputs.5'`: The layer of interest we want to generate attention maps from. \n",
        "\n",
        "All the possible parameters of `medcam.inject` are explained in the documentation as well.\n",
        "\n",
        "\n",
        "M3d-CAM has also the ability to evaluate the attention maps if the corresponding ground truth masks are given. For completion, the code needed for the evaluation is shown below:\n",
        "\n",
        "```\n",
        "# Line 34\n",
        "from medcam import medcam\n",
        "# Line 188\n",
        "evaluator = medcam.Evaluator(\"/content/medcam_results\")\n",
        "# Line 189\n",
        "trainer.network = medcam.inject(trainer.network, label=1, replace=True, backend=\"gcam\", layer='seg_outputs.5')\n",
        "trainer.network.inference_apply_nonlin = lambda x: x \n",
        "# Original inference code here...\n",
        "# ...\n",
        "# ...\n",
        "# Evaluation after inference:\n",
        "# Line 244 (Replace with the original)\n",
        "filenames = [i.get() for i in results]\n",
        "# Line 246\n",
        "evaluate(evaluator, filenames, layer)\n",
        "# Line 247\n",
        "evaluator.dump()\n",
        "# Line 274-285\n",
        "def evaluate(evaluator, filenames, layer):\n",
        "    filenames = np.asarray(filenames).squeeze()\n",
        "    for filename in filenames:\n",
        "        attention_map = np.array(nib.load(filename).dataobj)\n",
        "        mask_name = filename[-21:]\n",
        "        mask_name = mask_name[:11] + mask_name[-7:]\n",
        "        class_label = int(filename[-8])\n",
        "        mask = np.array(nib.load(\"/content/nnUNet_raw_data_base/nnUNet_raw_data/nnUNet_raw_data/Task005_Prostate/labelsTr/\" + mask_name).dataobj)\n",
        "        mask[mask != class_label] = -1\n",
        "        mask[mask == class_label] = 1\n",
        "        mask[mask != 1] = 0\n",
        "        evaluator.comp_score(attention_map, mask, layer=layer, class_label=class_label, name=mask_name)\n",
        "\n",
        "```\n",
        "\n",
        "*Note: Because we are returning the attention maps instead of the nnUNet predictions the evaluation cannot be done by M3d-CAM internally. Instead we have to use M3d-CAM's external evaluator which results in more code than usual for the evaluation to work.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsmah0PfnKhp",
        "colab_type": "text"
      },
      "source": [
        "The following cell includes the already modified predict.py. By running the cell once the original predict.py will be replaced."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3Ya0ke6gTOf",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Modified predict.py - Replaces original predict.py\n",
        "%%writefile /content/nnUNet/nnunet/inference/predict.py\n",
        "#    Copyright 2020 Division of Medical Image Computing, German Cancer Research Center (DKFZ), Heidelberg, Germany\n",
        "#\n",
        "#    Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "#    you may not use this file except in compliance with the License.\n",
        "#    You may obtain a copy of the License at\n",
        "#\n",
        "#        http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "#    Unless required by applicable law or agreed to in writing, software\n",
        "#    distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "#    See the License for the specific language governing permissions and\n",
        "#    limitations under the License.\n",
        "\n",
        "\n",
        "import argparse\n",
        "from copy import deepcopy\n",
        "\n",
        "import numpy as np\n",
        "from batchgenerators.augmentations.utils import resize_segmentation\n",
        "from nnunet.inference.segmentation_export import save_segmentation_nifti_from_softmax, save_segmentation_nifti\n",
        "from batchgenerators.utilities.file_and_folder_operations import *\n",
        "from multiprocessing import Process, Queue\n",
        "import torch\n",
        "import SimpleITK as sitk\n",
        "import shutil\n",
        "from multiprocessing import Pool\n",
        "from nnunet.postprocessing.connected_components import load_remove_save, load_postprocessing\n",
        "from nnunet.training.model_restore import load_model_and_checkpoint_files\n",
        "from nnunet.training.network_training.nnUNetTrainer import nnUNetTrainer\n",
        "from nnunet.utilities.one_hot_encoding import to_one_hot\n",
        "from medcam import medcam\n",
        "\n",
        "\n",
        "def preprocess_save_to_queue(preprocess_fn, q, list_of_lists, output_files, segs_from_prev_stage, classes, transpose_forward):\n",
        "    # suppress output\n",
        "    #sys.stdout = open(os.devnull, 'w')\n",
        "\n",
        "    errors_in = []\n",
        "    for i, l in enumerate(list_of_lists):\n",
        "        try:\n",
        "            output_file = output_files[i]\n",
        "            print(\"preprocessing\", output_file)\n",
        "            d, _, dct = preprocess_fn(l)\n",
        "            # print(output_file, dct)\n",
        "            if segs_from_prev_stage[i] is not None:\n",
        "                assert isfile(segs_from_prev_stage[i]) and segs_from_prev_stage[i].endswith(\n",
        "                    \".nii.gz\"), \"segs_from_prev_stage\" \\\n",
        "                                \" must point to a \" \\\n",
        "                                \"segmentation file\"\n",
        "                seg_prev = sitk.GetArrayFromImage(sitk.ReadImage(segs_from_prev_stage[i]))\n",
        "                # check to see if shapes match\n",
        "                img = sitk.GetArrayFromImage(sitk.ReadImage(l[0]))\n",
        "                assert all([i == j for i, j in zip(seg_prev.shape, img.shape)]), \"image and segmentation from previous \" \\\n",
        "                                                                                 \"stage don't have the same pixel array \" \\\n",
        "                                                                                 \"shape! image: %s, seg_prev: %s\" % \\\n",
        "                                                                                 (l[0], segs_from_prev_stage[i])\n",
        "                seg_prev = seg_prev.transpose(transpose_forward)\n",
        "                seg_reshaped = resize_segmentation(seg_prev, d.shape[1:], order=1, cval=0)\n",
        "                seg_reshaped = to_one_hot(seg_reshaped, classes)\n",
        "                d = np.vstack((d, seg_reshaped)).astype(np.float32)\n",
        "            \"\"\"There is a problem with python process communication that prevents us from communicating obejcts \n",
        "            larger than 2 GB between processes (basically when the length of the pickle string that will be sent is \n",
        "            communicated by the multiprocessing.Pipe object then the placeholder (\\%i I think) does not allow for long \n",
        "            enough strings (lol). This could be fixed by changing i to l (for long) but that would require manually \n",
        "            patching system python code. We circumvent that problem here by saving softmax_pred to a npy file that will \n",
        "            then be read (and finally deleted) by the Process. save_segmentation_nifti_from_softmax can take either \n",
        "            filename or np.ndarray and will handle this automatically\"\"\"\n",
        "            print(d.shape)\n",
        "            if np.prod(d.shape) > (2e9 / 4 * 0.85):  # *0.85 just to be save, 4 because float32 is 4 bytes\n",
        "                print(\n",
        "                    \"This output is too large for python process-process communication. \"\n",
        "                    \"Saving output temporarily to disk\")\n",
        "                np.save(output_file[:-7] + \".npy\", d)\n",
        "                d = output_file[:-7] + \".npy\"\n",
        "            q.put((output_file, (d, dct)))\n",
        "        except KeyboardInterrupt:\n",
        "            raise KeyboardInterrupt\n",
        "        except Exception as e:\n",
        "            print(\"error in\", l)\n",
        "            print(e)\n",
        "    q.put(\"end\")\n",
        "    if len(errors_in) > 0:\n",
        "        print(\"There were some errors in the following cases:\", errors_in)\n",
        "        print(\"These cases were ignored.\")\n",
        "    else:\n",
        "        print(\"This worker has ended successfully, no errors to report\")\n",
        "    # restore output\n",
        "    #sys.stdout = sys.__stdout__\n",
        "\n",
        "\n",
        "def preprocess_multithreaded(trainer, list_of_lists, output_files, num_processes=2, segs_from_prev_stage=None):\n",
        "    if segs_from_prev_stage is None:\n",
        "        segs_from_prev_stage = [None] * len(list_of_lists)\n",
        "\n",
        "    num_processes = min(len(list_of_lists), num_processes)\n",
        "\n",
        "    classes = list(range(1, trainer.num_classes))\n",
        "    assert isinstance(trainer, nnUNetTrainer)\n",
        "    q = Queue(2)\n",
        "    processes = []\n",
        "    for i in range(num_processes):\n",
        "        pr = Process(target=preprocess_save_to_queue, args=(trainer.preprocess_patient, q,\n",
        "                                                         list_of_lists[i::num_processes],\n",
        "                                                         output_files[i::num_processes],\n",
        "                                                         segs_from_prev_stage[i::num_processes],\n",
        "                                                         classes, trainer.plans['transpose_forward']))\n",
        "        pr.start()\n",
        "        processes.append(pr)\n",
        "\n",
        "    try:\n",
        "        end_ctr = 0\n",
        "        while end_ctr != num_processes:\n",
        "            item = q.get()\n",
        "            if item == \"end\":\n",
        "                end_ctr += 1\n",
        "                continue\n",
        "            else:\n",
        "                yield item\n",
        "\n",
        "    finally:\n",
        "        for p in processes:\n",
        "            if p.is_alive():\n",
        "                p.terminate()  # this should not happen but better safe than sorry right\n",
        "            p.join()\n",
        "\n",
        "        q.close()\n",
        "\n",
        "\n",
        "def predict_cases(model, list_of_lists, output_filenames, folds, save_npz, num_threads_preprocessing,\n",
        "                  num_threads_nifti_save, segs_from_prev_stage=None, do_tta=True, fp16=None, overwrite_existing=False,\n",
        "                  all_in_gpu=False, step_size=0.5, force_separate_z=None, interp_order=3, interp_order_z=0,\n",
        "                  checkpoint_name=\"model_final_checkpoint\"):\n",
        "    \"\"\"\n",
        "\n",
        "    :param model:\n",
        "    :param list_of_lists:\n",
        "    :param output_filenames:\n",
        "    :param folds:\n",
        "    :param save_npz:\n",
        "    :param num_threads_preprocessing:\n",
        "    :param num_threads_nifti_save:\n",
        "    :param segs_from_prev_stage:\n",
        "    :param do_tta:\n",
        "    :param overwrite_existing:\n",
        "    :param fp16: if None then we take no action. If True/False we overwrite what the model has in its init\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    assert len(list_of_lists) == len(output_filenames)\n",
        "    if segs_from_prev_stage is not None: assert len(segs_from_prev_stage) == len(output_filenames)\n",
        "\n",
        "    pool = Pool(num_threads_nifti_save)\n",
        "    results = []\n",
        "\n",
        "    cleaned_output_files = []\n",
        "    for o in output_filenames:\n",
        "        dr, f = os.path.split(o)\n",
        "        if len(dr) > 0:\n",
        "            maybe_mkdir_p(dr)\n",
        "        if not f.endswith(\".nii.gz\"):\n",
        "            f, _ = os.path.splitext(f)\n",
        "            f = f + \".nii.gz\"\n",
        "        cleaned_output_files.append(join(dr, f))\n",
        "\n",
        "    if not overwrite_existing:\n",
        "        print(\"number of cases:\", len(list_of_lists))\n",
        "        not_done_idx = [i for i, j in enumerate(cleaned_output_files) if not isfile(j)]\n",
        "\n",
        "        cleaned_output_files = [cleaned_output_files[i] for i in not_done_idx]\n",
        "        list_of_lists = [list_of_lists[i] for i in not_done_idx]\n",
        "        if segs_from_prev_stage is not None:\n",
        "            segs_from_prev_stage = [segs_from_prev_stage[i] for i in not_done_idx]\n",
        "\n",
        "        print(\"number of cases that still need to be predicted:\", len(cleaned_output_files))\n",
        "\n",
        "    print(\"emptying cuda cache\")\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    print(\"loading parameters for folds,\", folds)\n",
        "    trainer, params = load_model_and_checkpoint_files(model, folds, fp16=fp16, checkpoint_name=checkpoint_name)\n",
        "\n",
        "    print(\"starting preprocessing generator\")\n",
        "    preprocessing = preprocess_multithreaded(trainer, list_of_lists, cleaned_output_files, num_threads_preprocessing,\n",
        "                                             segs_from_prev_stage)\n",
        "\n",
        "    evaluator = medcam.Evaluator(\"/content/medcam_results\")\n",
        "    trainer.network = medcam.inject(trainer.network, output_dir=\"/content/medcam_results\", label=1, replace=True, backend=\"gcam\", layer='seg_outputs.5')\n",
        "\n",
        "    print(\"starting prediction...\")\n",
        "    all_output_files = []\n",
        "    for preprocessed in preprocessing:\n",
        "        output_filename, (d, dct) = preprocessed\n",
        "        all_output_files.append(all_output_files)\n",
        "        if isinstance(d, str):\n",
        "            data = np.load(d)\n",
        "            os.remove(d)\n",
        "            d = data\n",
        "\n",
        "        print(\"predicting\", output_filename)\n",
        "        softmax = []\n",
        "        for p in params:\n",
        "            trainer.load_checkpoint_ram(p, False)\n",
        "            softmax.append(trainer.predict_preprocessed_data_return_seg_and_softmax(\n",
        "                d, do_tta, trainer.data_aug_params['mirror_axes'], True, step_size=step_size, use_gaussian=True,\n",
        "                all_in_gpu=all_in_gpu)[1][None])\n",
        "\n",
        "        softmax = np.vstack(softmax)\n",
        "        softmax_mean = np.mean(softmax, 0)\n",
        "\n",
        "        transpose_forward = trainer.plans.get('transpose_forward')\n",
        "        if transpose_forward is not None:\n",
        "            transpose_backward = trainer.plans.get('transpose_backward')\n",
        "            softmax_mean = softmax_mean.transpose([0] + [i + 1 for i in transpose_backward])\n",
        "\n",
        "        if save_npz:\n",
        "            npz_file = output_filename[:-7] + \".npz\"\n",
        "        else:\n",
        "            npz_file = None\n",
        "\n",
        "        \"\"\"There is a problem with python process communication that prevents us from communicating obejcts \n",
        "        larger than 2 GB between processes (basically when the length of the pickle string that will be sent is \n",
        "        communicated by the multiprocessing.Pipe object then the placeholder (\\%i I think) does not allow for long \n",
        "        enough strings (lol). This could be fixed by changing i to l (for long) but that would require manually \n",
        "        patching system python code. We circumvent that problem here by saving softmax_pred to a npy file that will \n",
        "        then be read (and finally deleted) by the Process. save_segmentation_nifti_from_softmax can take either \n",
        "        filename or np.ndarray and will handle this automatically\"\"\"\n",
        "        bytes_per_voxel = 4\n",
        "        if all_in_gpu:\n",
        "            bytes_per_voxel = 2 # if all_in_gpu then the return value is half (float16)\n",
        "        if np.prod(softmax_mean.shape) > (2e9 / bytes_per_voxel * 0.85):  # * 0.85 just to be save\n",
        "            print(\n",
        "                \"This output is too large for python process-process communication. Saving output temporarily to disk\")\n",
        "            np.save(output_filename[:-7] + \".npy\", softmax_mean)\n",
        "            softmax_mean = output_filename[:-7] + \".npy\"\n",
        "\n",
        "        results.append(pool.starmap_async(save_segmentation_nifti_from_softmax,\n",
        "                                          ((softmax_mean, output_filename, dct, interp_order, None, None, None,\n",
        "                                            npz_file, None, force_separate_z, interp_order_z),)\n",
        "                                          ))\n",
        "\n",
        "    print(\"inference done. Now waiting for the segmentation export to finish...\")\n",
        "    filenames = [i.get() for i in results]\n",
        "\n",
        "    evaluate(evaluator, filenames, layer)\n",
        "    evaluator.dump()\n",
        "\n",
        "\n",
        "    # now apply postprocessing\n",
        "    # first load the postprocessing properties if they are present. Else raise a well visible warning\n",
        "    results = []\n",
        "    pp_file = join(model, \"postprocessing.json\")\n",
        "    if isfile(pp_file):\n",
        "        print(\"postprocessing...\")\n",
        "        shutil.copy(pp_file, os.path.abspath(os.path.dirname(output_filenames[0])))\n",
        "        # for_which_classes stores for which of the classes everything but the largest connected component needs to be\n",
        "        # removed\n",
        "        for_which_classes, min_valid_obj_size = load_postprocessing(pp_file)\n",
        "        results.append(pool.starmap_async(load_remove_save,\n",
        "                                          zip(output_filenames, output_filenames,\n",
        "                                              [for_which_classes] * len(output_filenames),\n",
        "                                              [min_valid_obj_size] * len(output_filenames))))\n",
        "        _ = [i.get() for i in results]\n",
        "    else:\n",
        "        print(\"WARNING! Cannot run postprocessing because the postprocessing file is missing. Make sure to run \"\n",
        "              \"consolidate_folds in the output folder of the model first!\\nThe folder you need to run this in is \"\n",
        "              \"%s\" % model)\n",
        "\n",
        "    pool.close()\n",
        "    pool.join()\n",
        "\n",
        "\n",
        "def evaluate(evaluator, filenames, layer):\n",
        "    filenames = np.asarray(filenames).squeeze()\n",
        "    for filename in filenames:\n",
        "        attention_map = np.array(nib.load(filename).dataobj)\n",
        "        mask_name = filename[-21:]\n",
        "        mask_name = mask_name[:11] + mask_name[-7:]\n",
        "        class_label = int(filename[-8])\n",
        "        mask = np.array(nib.load(\"/content/nnUNet_raw_data_base/nnUNet_raw_data/nnUNet_raw_data/Task005_Prostate/labelsTr/\" + mask_name).dataobj)\n",
        "        mask[mask != class_label] = -1\n",
        "        mask[mask == class_label] = 1\n",
        "        mask[mask != 1] = 0\n",
        "        evaluator.comp_score(attention_map, mask, layer=layer, class_label=class_label, name=mask_name)\n",
        "\n",
        "\n",
        "def predict_cases_fast(model, list_of_lists, output_filenames, folds, num_threads_preprocessing,\n",
        "                       num_threads_nifti_save, segs_from_prev_stage=None, do_tta=True, fp16=None,\n",
        "                       overwrite_existing=False, all_in_gpu=True, step_size=0.5, checkpoint_name=\"model_final_checkpoint\",\n",
        "                       force_separate_z=None, interp_order=3):\n",
        "    assert len(list_of_lists) == len(output_filenames)\n",
        "    if segs_from_prev_stage is not None: assert len(segs_from_prev_stage) == len(output_filenames)\n",
        "\n",
        "    pool = Pool(num_threads_nifti_save)\n",
        "    results = []\n",
        "\n",
        "    cleaned_output_files = []\n",
        "    for o in output_filenames:\n",
        "        dr, f = os.path.split(o)\n",
        "        if len(dr) > 0:\n",
        "            maybe_mkdir_p(dr)\n",
        "        if not f.endswith(\".nii.gz\"):\n",
        "            f, _ = os.path.splitext(f)\n",
        "            f = f + \".nii.gz\"\n",
        "        cleaned_output_files.append(join(dr, f))\n",
        "\n",
        "    if not overwrite_existing:\n",
        "        print(\"number of cases:\", len(list_of_lists))\n",
        "        not_done_idx = [i for i, j in enumerate(cleaned_output_files) if not isfile(j)]\n",
        "\n",
        "        cleaned_output_files = [cleaned_output_files[i] for i in not_done_idx]\n",
        "        list_of_lists = [list_of_lists[i] for i in not_done_idx]\n",
        "        if segs_from_prev_stage is not None:\n",
        "            segs_from_prev_stage = [segs_from_prev_stage[i] for i in not_done_idx]\n",
        "\n",
        "        print(\"number of cases that still need to be predicted:\", len(cleaned_output_files))\n",
        "\n",
        "    print(\"emptying cuda cache\")\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    print(\"loading parameters for folds,\", folds)\n",
        "    trainer, params = load_model_and_checkpoint_files(model, folds, fp16=fp16, checkpoint_name=checkpoint_name)\n",
        "\n",
        "    print(\"starting preprocessing generator\")\n",
        "    preprocessing = preprocess_multithreaded(trainer, list_of_lists, cleaned_output_files, num_threads_preprocessing,\n",
        "                                             segs_from_prev_stage)\n",
        "\n",
        "    print(\"starting prediction...\")\n",
        "    for preprocessed in preprocessing:\n",
        "        print(\"getting data from preprocessor\")\n",
        "        output_filename, (d, dct) = preprocessed\n",
        "        print(\"got something\")\n",
        "        if isinstance(d, str):\n",
        "            print(\"what I got is a string, so I need to load a file\")\n",
        "            data = np.load(d)\n",
        "            os.remove(d)\n",
        "            d = data\n",
        "\n",
        "        # preallocate the output arrays\n",
        "        # same dtype as the return value in predict_preprocessed_data_return_seg_and_softmax (saves time)\n",
        "        softmax_aggr = None # np.zeros((trainer.num_classes, *d.shape[1:]), dtype=np.float16)\n",
        "        all_seg_outputs = np.zeros((len(params), *d.shape[1:]), dtype=int)\n",
        "        print(\"predicting\", output_filename)\n",
        "\n",
        "        for i, p in enumerate(params):\n",
        "            trainer.load_checkpoint_ram(p, False)\n",
        "\n",
        "            res = trainer.predict_preprocessed_data_return_seg_and_softmax(\n",
        "                d, do_tta, trainer.data_aug_params['mirror_axes'], True, step_size=step_size, use_gaussian=True,\n",
        "                all_in_gpu=all_in_gpu)[1]\n",
        "\n",
        "            if len(params) > 1:\n",
        "                # otherwise we dont need this and we can save ourselves the time it takes to copy that\n",
        "                print(\"aggregating softmax\")\n",
        "                if softmax_aggr is None:\n",
        "                    softmax_aggr = res[1]\n",
        "                else:\n",
        "                    softmax_aggr += res[1]\n",
        "            all_seg_outputs[i] = res[0]\n",
        "\n",
        "        print(\"obtaining segmentation map\")\n",
        "        if len(params) > 1:\n",
        "            # we dont need to normalize the softmax by 1 / len(params) because this would not change the outcome of the argmax\n",
        "            seg = softmax_aggr.argmax(0)\n",
        "        else:\n",
        "            seg = all_seg_outputs[0]\n",
        "\n",
        "        print(\"applying transpose_backward\")\n",
        "        transpose_forward = trainer.plans.get('transpose_forward')\n",
        "        if transpose_forward is not None:\n",
        "            transpose_backward = trainer.plans.get('transpose_backward')\n",
        "            seg = seg.transpose([i for i in transpose_backward])\n",
        "\n",
        "        print(\"initializing segmentation export\")\n",
        "        results.append(pool.starmap_async(save_segmentation_nifti,\n",
        "                                           ((seg, output_filename, dct, interp_order, force_separate_z),)\n",
        "                                           ))\n",
        "        print(\"done\")\n",
        "\n",
        "    print(\"inference done. Now waiting for the segmentation export to finish...\")\n",
        "    _ = [i.get() for i in results]\n",
        "    # now apply postprocessing\n",
        "    # first load the postprocessing properties if they are present. Else raise a well visible warning\n",
        "    results = []\n",
        "    pp_file = join(model, \"postprocessing.json\")\n",
        "    if isfile(pp_file):\n",
        "        print(\"postprocessing...\")\n",
        "        shutil.copy(pp_file, os.path.dirname(output_filenames[0]))\n",
        "        # for_which_classes stores for which of the classes everything but the largest connected component needs to be\n",
        "        # removed\n",
        "        for_which_classes, min_valid_obj_size = load_postprocessing(pp_file)\n",
        "        results.append(pool.starmap_async(load_remove_save,\n",
        "                                          zip(output_filenames, output_filenames,\n",
        "                                              [for_which_classes] * len(output_filenames),\n",
        "                                              [min_valid_obj_size] * len(output_filenames))))\n",
        "        _ = [i.get() for i in results]\n",
        "    else:\n",
        "        print(\"WARNING! Cannot run postprocessing because the postprocessing file is missing. Make sure to run \"\n",
        "              \"consolidate_folds in the output folder of the model first!\\nThe folder you need to run this in is \"\n",
        "              \"%s\" % model)\n",
        "\n",
        "    pool.close()\n",
        "    pool.join()\n",
        "\n",
        "\n",
        "def predict_cases_fastest(model, list_of_lists, output_filenames, folds, num_threads_preprocessing,\n",
        "                          num_threads_nifti_save, segs_from_prev_stage=None, do_tta=True, fp16=None,\n",
        "                          overwrite_existing=False, all_in_gpu=True, step_size=0.5,\n",
        "                          checkpoint_name=\"model_final_checkpoint\"):\n",
        "    assert len(list_of_lists) == len(output_filenames)\n",
        "    if segs_from_prev_stage is not None: assert len(segs_from_prev_stage) == len(output_filenames)\n",
        "\n",
        "    pool = Pool(num_threads_nifti_save)\n",
        "    results = []\n",
        "\n",
        "    cleaned_output_files = []\n",
        "    for o in output_filenames:\n",
        "        dr, f = os.path.split(o)\n",
        "        if len(dr) > 0:\n",
        "            maybe_mkdir_p(dr)\n",
        "        if not f.endswith(\".nii.gz\"):\n",
        "            f, _ = os.path.splitext(f)\n",
        "            f = f + \".nii.gz\"\n",
        "        cleaned_output_files.append(join(dr, f))\n",
        "\n",
        "    if not overwrite_existing:\n",
        "        print(\"number of cases:\", len(list_of_lists))\n",
        "        not_done_idx = [i for i, j in enumerate(cleaned_output_files) if not isfile(j)]\n",
        "\n",
        "        cleaned_output_files = [cleaned_output_files[i] for i in not_done_idx]\n",
        "        list_of_lists = [list_of_lists[i] for i in not_done_idx]\n",
        "        if segs_from_prev_stage is not None:\n",
        "            segs_from_prev_stage = [segs_from_prev_stage[i] for i in not_done_idx]\n",
        "\n",
        "        print(\"number of cases that still need to be predicted:\", len(cleaned_output_files))\n",
        "\n",
        "    print(\"emptying cuda cache\")\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    print(\"loading parameters for folds,\", folds)\n",
        "    trainer, params = load_model_and_checkpoint_files(model, folds, fp16=fp16, checkpoint_name=checkpoint_name)\n",
        "\n",
        "    print(\"starting preprocessing generator\")\n",
        "    preprocessing = preprocess_multithreaded(trainer, list_of_lists, cleaned_output_files, num_threads_preprocessing,\n",
        "                                             segs_from_prev_stage)\n",
        "\n",
        "    print(\"starting prediction...\")\n",
        "    for preprocessed in preprocessing:\n",
        "        print(\"getting data from preprocessor\")\n",
        "        output_filename, (d, dct) = preprocessed\n",
        "        print(\"got something\")\n",
        "        if isinstance(d, str):\n",
        "            print(\"what I got is a string, so I need to load a file\")\n",
        "            data = np.load(d)\n",
        "            os.remove(d)\n",
        "            d = data\n",
        "\n",
        "        # preallocate the output arrays\n",
        "        # same dtype as the return value in predict_preprocessed_data_return_seg_and_softmax (saves time)\n",
        "        all_softmax_outputs = np.zeros((len(params), trainer.num_classes, *d.shape[1:]), dtype=np.float16)\n",
        "        all_seg_outputs = np.zeros((len(params), *d.shape[1:]), dtype=int)\n",
        "        print(\"predicting\", output_filename)\n",
        "\n",
        "        for i, p in enumerate(params):\n",
        "            trainer.load_checkpoint_ram(p, False)\n",
        "            res = trainer.predict_preprocessed_data_return_seg_and_softmax(\n",
        "                d, do_tta, trainer.data_aug_params['mirror_axes'], True, step_size=step_size, use_gaussian=True,\n",
        "                all_in_gpu=all_in_gpu\n",
        "            )[1]\n",
        "            if len(params) > 1:\n",
        "                # otherwise we dont need this and we can save ourselves the time it takes to copy that\n",
        "                all_softmax_outputs[i] = res[1]\n",
        "            all_seg_outputs[i] = res[0]\n",
        "\n",
        "        print(\"aggregating predictions\")\n",
        "        if len(params) > 1:\n",
        "            softmax_mean = np.mean(all_softmax_outputs, 0)\n",
        "            seg = softmax_mean.argmax(0)\n",
        "        else:\n",
        "            seg = all_seg_outputs[0]\n",
        "\n",
        "        print(\"applying transpose_backward\")\n",
        "        transpose_forward = trainer.plans.get('transpose_forward')\n",
        "        if transpose_forward is not None:\n",
        "            transpose_backward = trainer.plans.get('transpose_backward')\n",
        "            seg = seg.transpose([i for i in transpose_backward])\n",
        "\n",
        "        print(\"initializing segmentation export\")\n",
        "        results.append(pool.starmap_async(save_segmentation_nifti,\n",
        "                                           ((seg, output_filename, dct, 0, None),)\n",
        "                                           ))\n",
        "        print(\"done\")\n",
        "\n",
        "    print(\"inference done. Now waiting for the segmentation export to finish...\")\n",
        "    _ = [i.get() for i in results]\n",
        "    # now apply postprocessing\n",
        "    # first load the postprocessing properties if they are present. Else raise a well visible warning\n",
        "    results = []\n",
        "    pp_file = join(model, \"postprocessing.json\")\n",
        "    if isfile(pp_file):\n",
        "        print(\"postprocessing...\")\n",
        "        shutil.copy(pp_file, os.path.dirname(output_filenames[0]))\n",
        "        # for_which_classes stores for which of the classes everything but the largest connected component needs to be\n",
        "        # removed\n",
        "        for_which_classes, min_valid_obj_size = load_postprocessing(pp_file)\n",
        "        results.append(pool.starmap_async(load_remove_save,\n",
        "                                          zip(output_filenames, output_filenames,\n",
        "                                              [for_which_classes] * len(output_filenames),\n",
        "                                              [min_valid_obj_size] * len(output_filenames))))\n",
        "        _ = [i.get() for i in results]\n",
        "    else:\n",
        "        print(\"WARNING! Cannot run postprocessing because the postprocessing file is missing. Make sure to run \"\n",
        "              \"consolidate_folds in the output folder of the model first!\\nThe folder you need to run this in is \"\n",
        "              \"%s\" % model)\n",
        "\n",
        "    pool.close()\n",
        "    pool.join()\n",
        "\n",
        "\n",
        "def check_input_folder_and_return_caseIDs(input_folder, expected_num_modalities):\n",
        "    print(\"This model expects %d input modalities for each image\" % expected_num_modalities)\n",
        "    files = subfiles(input_folder, suffix=\".nii.gz\", join=False, sort=True)\n",
        "\n",
        "    maybe_case_ids = np.unique([i[:-12] for i in files])\n",
        "\n",
        "    remaining = deepcopy(files)\n",
        "    missing = []\n",
        "\n",
        "    assert len(files) > 0, \"input folder did not contain any images (expected to find .nii.gz file endings)\"\n",
        "\n",
        "    # now check if all required files are present and that no unexpected files are remaining\n",
        "    for c in maybe_case_ids:\n",
        "        for n in range(expected_num_modalities):\n",
        "            expected_output_file = c + \"_%04.0d.nii.gz\" % n\n",
        "            if not isfile(join(input_folder, expected_output_file)):\n",
        "                missing.append(expected_output_file)\n",
        "            else:\n",
        "                remaining.remove(expected_output_file)\n",
        "\n",
        "    print(\"Found %d unique case ids, here are some examples:\" % len(maybe_case_ids), np.random.choice(maybe_case_ids, min(len(maybe_case_ids), 10)))\n",
        "    print(\"If they don't look right, make sure to double check your filenames. They must end with _0000.nii.gz etc\")\n",
        "\n",
        "    if len(remaining) > 0:\n",
        "        print(\"found %d unexpected remaining files in the folder. Here are some examples:\" % len(remaining), np.random.choice(remaining, min(len(remaining), 10)))\n",
        "\n",
        "    if len(missing) > 0:\n",
        "        print(\"Some files are missing:\")\n",
        "        print(missing)\n",
        "        raise RuntimeError(\"missing files in input_folder\")\n",
        "\n",
        "    return maybe_case_ids\n",
        "\n",
        "\n",
        "def predict_from_folder(model, input_folder, output_folder, folds, save_npz, num_threads_preprocessing,\n",
        "                        num_threads_nifti_save, lowres_segmentations, part_id, num_parts, tta, fp16=False,\n",
        "                        overwrite_existing=True, mode='normal', overwrite_all_in_gpu=None, step_size: float = 0.5,\n",
        "                        force_separate_z=None, interp_order=3, interp_order_z=0, \n",
        "                        checkpoint_name=\"model_final_checkpoint\"):\n",
        "    \"\"\"\n",
        "        here we use the standard naming scheme to generate list_of_lists and output_files needed by predict_cases\n",
        "\n",
        "    :param model:\n",
        "    :param input_folder:\n",
        "    :param output_folder:\n",
        "    :param folds:\n",
        "    :param save_npz:\n",
        "    :param num_threads_preprocessing:\n",
        "    :param num_threads_nifti_save:\n",
        "    :param lowres_segmentations:\n",
        "    :param part_id:\n",
        "    :param num_parts:\n",
        "    :param tta:\n",
        "    :param fp16:\n",
        "    :param overwrite_existing: if not None then it will be overwritten with whatever is in there. None is default (no overwrite)\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    maybe_mkdir_p(output_folder)\n",
        "    shutil.copy(join(model, 'plans.pkl'), output_folder)\n",
        "\n",
        "    assert isfile(join(model, \"plans.pkl\")), \"Folder with saved model weights must contain a plans.pkl file\"\n",
        "    expected_num_modalities = load_pickle(join(model, \"plans.pkl\"))['num_modalities']\n",
        "\n",
        "    # check input folder integrity\n",
        "    case_ids = check_input_folder_and_return_caseIDs(input_folder, expected_num_modalities)\n",
        "\n",
        "    output_files = [join(output_folder, i + \".nii.gz\") for i in case_ids]\n",
        "    all_files = subfiles(input_folder, suffix=\".nii.gz\", join=False, sort=True)\n",
        "    list_of_lists = [[join(input_folder, i) for i in all_files if i[:len(j)].startswith(j) and\n",
        "                      len(i) == (len(j) + 12)] for j in case_ids]\n",
        "\n",
        "    if lowres_segmentations is not None:\n",
        "        assert isdir(lowres_segmentations), \"if lowres_segmentations is not None then it must point to a directory\"\n",
        "        lowres_segmentations = [join(lowres_segmentations, i + \".nii.gz\") for i in case_ids]\n",
        "        assert all([isfile(i) for i in lowres_segmentations]), \"not all lowres_segmentations files are present. \" \\\n",
        "                                                               \"(I was searching for case_id.nii.gz in that folder)\"\n",
        "        lowres_segmentations = lowres_segmentations[part_id::num_parts]\n",
        "    else:\n",
        "        lowres_segmentations = None\n",
        "\n",
        "    if mode == \"normal\":\n",
        "        if overwrite_all_in_gpu is None:\n",
        "            all_in_gpu = False\n",
        "        else:\n",
        "            all_in_gpu = overwrite_all_in_gpu\n",
        "\n",
        "        return predict_cases(model, list_of_lists[part_id::num_parts], output_files[part_id::num_parts], folds,\n",
        "                             save_npz,\n",
        "                             num_threads_preprocessing, num_threads_nifti_save, lowres_segmentations,\n",
        "                             tta, fp16=fp16, overwrite_existing=overwrite_existing, all_in_gpu=all_in_gpu, step_size=step_size,\n",
        "                             force_separate_z=force_separate_z, interp_order=interp_order, interp_order_z=interp_order_z,\n",
        "                             checkpoint_name=checkpoint_name)\n",
        "    elif mode == \"fast\":\n",
        "        if overwrite_all_in_gpu is None:\n",
        "            all_in_gpu = True\n",
        "        else:\n",
        "            all_in_gpu = overwrite_all_in_gpu\n",
        "\n",
        "        assert save_npz is False\n",
        "        return predict_cases_fast(model, list_of_lists[part_id::num_parts], output_files[part_id::num_parts], folds,\n",
        "                                  num_threads_preprocessing, num_threads_nifti_save, lowres_segmentations,\n",
        "                                  tta, fp16=fp16, overwrite_existing=overwrite_existing, all_in_gpu=all_in_gpu, step_size=step_size,\n",
        "                                  force_separate_z=force_separate_z, interp_order=interp_order, \n",
        "                                  checkpoint_name=checkpoint_name)\n",
        "    elif mode == \"fastest\":\n",
        "        if overwrite_all_in_gpu is None:\n",
        "            all_in_gpu = True\n",
        "        else:\n",
        "            all_in_gpu = overwrite_all_in_gpu\n",
        "\n",
        "        assert save_npz is False\n",
        "        return predict_cases_fastest(model, list_of_lists[part_id::num_parts], output_files[part_id::num_parts], folds,\n",
        "                                     num_threads_preprocessing, num_threads_nifti_save, lowres_segmentations,\n",
        "                                     tta, fp16=fp16, overwrite_existing=overwrite_existing, all_in_gpu=all_in_gpu, \n",
        "                                     step_size=step_size, checkpoint_name=checkpoint_name)\n",
        "    else:\n",
        "        raise ValueError(\"unrecognized mode. Must be normal, fast or fastest\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"-i\", '--input_folder', help=\"Must contain all modalities for each patient in the correct\"\n",
        "                                                     \" order (same as training). Files must be named \"\n",
        "                                                     \"CASENAME_XXXX.nii.gz where XXXX is the modality \"\n",
        "                                                     \"identifier (0000, 0001, etc)\", required=True)\n",
        "    parser.add_argument('-o', \"--output_folder\", required=True, help=\"folder for saving predictions\")\n",
        "    parser.add_argument('-m', '--model_output_folder',\n",
        "                        help='model output folder. Will automatically discover the folds '\n",
        "                             'that were '\n",
        "                             'run and use those as an ensemble', required=True)\n",
        "    parser.add_argument('-f', '--folds', nargs='+', default='None', help=\"folds to use for prediction. Default is None \"\n",
        "                                                                         \"which means that folds will be detected \"\n",
        "                                                                         \"automatically in the model output folder\")\n",
        "    parser.add_argument('-z', '--save_npz', required=False, action='store_true', help=\"use this if you want to ensemble\"\n",
        "                                                                                      \" these predictions with those of\"\n",
        "                                                                                      \" other models. Softmax \"\n",
        "                                                                                      \"probabilities will be saved as \"\n",
        "                                                                                      \"compresed numpy arrays in \"\n",
        "                                                                                      \"output_folder and can be merged \"\n",
        "                                                                                      \"between output_folders with \"\n",
        "                                                                                      \"merge_predictions.py\")\n",
        "    parser.add_argument('-l', '--lowres_segmentations', required=False, default='None', help=\"if model is the highres \"\n",
        "                                                                                             \"stage of the cascade then you need to use -l to specify where the segmentations of the \"\n",
        "                                                                                             \"corresponding lowres unet are. Here they are required to do a prediction\")\n",
        "    parser.add_argument(\"--part_id\", type=int, required=False, default=0, help=\"Used to parallelize the prediction of \"\n",
        "                                                                               \"the folder over several GPUs. If you \"\n",
        "                                                                               \"want to use n GPUs to predict this \"\n",
        "                                                                               \"folder you need to run this command \"\n",
        "                                                                               \"n times with --part_id=0, ... n-1 and \"\n",
        "                                                                               \"--num_parts=n (each with a different \"\n",
        "                                                                               \"GPU (for example via \"\n",
        "                                                                               \"CUDA_VISIBLE_DEVICES=X)\")\n",
        "    parser.add_argument(\"--num_parts\", type=int, required=False, default=1,\n",
        "                        help=\"Used to parallelize the prediction of \"\n",
        "                             \"the folder over several GPUs. If you \"\n",
        "                             \"want to use n GPUs to predict this \"\n",
        "                             \"folder you need to run this command \"\n",
        "                             \"n times with --part_id=0, ... n-1 and \"\n",
        "                             \"--num_parts=n (each with a different \"\n",
        "                             \"GPU (via \"\n",
        "                             \"CUDA_VISIBLE_DEVICES=X)\")\n",
        "    parser.add_argument(\"--num_threads_preprocessing\", required=False, default=6, type=int, help=\n",
        "    \"Determines many background processes will be used for data preprocessing. Reduce this if you \"\n",
        "    \"run into out of memory (RAM) problems. Default: 6\")\n",
        "    parser.add_argument(\"--num_threads_nifti_save\", required=False, default=2, type=int, help=\n",
        "    \"Determines many background processes will be used for segmentation export. Reduce this if you \"\n",
        "    \"run into out of memory (RAM) problems. Default: 2\")\n",
        "    parser.add_argument(\"--tta\", required=False, type=int, default=1, help=\"Set to 0 to disable test time data \"\n",
        "                                                                           \"augmentation (speedup of factor \"\n",
        "                                                                           \"4(2D)/8(3D)), \"\n",
        "                                                                           \"lower quality segmentations\")\n",
        "    parser.add_argument(\"--fp16\", required=False, help=\"Flag for inference in FP16, default = off. DO NOT USE! It \"\n",
        "                                                       \"doesn't work\", action=\"store_true\")\n",
        "    parser.add_argument(\"--overwrite_existing\", required=False, type=int, default=1, help=\"Set this to 0 if you need \"\n",
        "                                                                                          \"to resume a previous \"\n",
        "                                                                                          \"prediction. Default: 1 \"\n",
        "                                                                                          \"(=existing segmentations \"\n",
        "                                                                                          \"in output_folder will be \"\n",
        "                                                                                          \"overwritten)\")\n",
        "    parser.add_argument(\"--mode\", type=str, default=\"normal\", required=False)\n",
        "    parser.add_argument(\"--all_in_gpu\", type=str, default=\"None\", required=False, help=\"can be None, False or True\")\n",
        "    parser.add_argument(\"--step_size\", type=float, default=0.5, required=False, help=\"don't touch\")\n",
        "    parser.add_argument(\"--interp_order\", required=False, default=3, type=int,\n",
        "                        help=\"order of interpolation for segmentations, has no effect if mode=fastest\")\n",
        "    parser.add_argument(\"--interp_order_z\", required=False, default=0, type=int,\n",
        "                        help=\"order of interpolation along z is z is done differently\")\n",
        "    parser.add_argument(\"--force_separate_z\", required=False, default=\"None\", type=str,\n",
        "                        help=\"force_separate_z resampling. Can be None, True or False, has no effect if mode=fastest\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "    input_folder = args.input_folder\n",
        "    output_folder = args.output_folder\n",
        "    part_id = args.part_id\n",
        "    num_parts = args.num_parts\n",
        "    model = args.model_output_folder\n",
        "    folds = args.folds\n",
        "    save_npz = args.save_npz\n",
        "    lowres_segmentations = args.lowres_segmentations\n",
        "    num_threads_preprocessing = args.num_threads_preprocessing\n",
        "    num_threads_nifti_save = args.num_threads_nifti_save\n",
        "    tta = args.tta\n",
        "    fp16 = args.fp16\n",
        "    step_size = args.step_size\n",
        "\n",
        "    interp_order = args.interp_order\n",
        "    interp_order_z = args.interp_order_z\n",
        "    force_separate_z = args.force_separate_z\n",
        "\n",
        "    if force_separate_z == \"None\":\n",
        "        force_separate_z = None\n",
        "    elif force_separate_z == \"False\":\n",
        "        force_separate_z = False\n",
        "    elif force_separate_z == \"True\":\n",
        "        force_separate_z = True\n",
        "    else:\n",
        "        raise ValueError(\"force_separate_z must be None, True or False. Given: %s\" % force_separate_z)\n",
        "\n",
        "    if fp16:\n",
        "        raise RuntimeError(\"FP16 support for inference does not work yet. Sorry :-/\")\n",
        "\n",
        "    overwrite = args.overwrite_existing\n",
        "    mode = args.mode\n",
        "    all_in_gpu = args.all_in_gpu\n",
        "\n",
        "    if lowres_segmentations == \"None\":\n",
        "        lowres_segmentations = None\n",
        "\n",
        "    if isinstance(folds, list):\n",
        "        if folds[0] == 'all' and len(folds) == 1:\n",
        "            pass\n",
        "        else:\n",
        "            folds = [int(i) for i in folds]\n",
        "    elif folds == \"None\":\n",
        "        folds = None\n",
        "    else:\n",
        "        raise ValueError(\"Unexpected value for argument folds\")\n",
        "\n",
        "    if tta == 0:\n",
        "        tta = False\n",
        "    elif tta == 1:\n",
        "        tta = True\n",
        "    else:\n",
        "        raise ValueError(\"Unexpected value for tta, Use 1 or 0\")\n",
        "\n",
        "    if overwrite == 0:\n",
        "        overwrite = False\n",
        "    elif overwrite == 1:\n",
        "        overwrite = True\n",
        "    else:\n",
        "        raise ValueError(\"Unexpected value for overwrite, Use 1 or 0\")\n",
        "\n",
        "    assert all_in_gpu in ['None', 'False', 'True']\n",
        "    if all_in_gpu == \"None\":\n",
        "        all_in_gpu = None\n",
        "    elif all_in_gpu == \"True\":\n",
        "        all_in_gpu = True\n",
        "    elif all_in_gpu == \"False\":\n",
        "        all_in_gpu = False\n",
        "\n",
        "    predict_from_folder(model, input_folder, output_folder, folds, save_npz, num_threads_preprocessing,\n",
        "                        num_threads_nifti_save, lowres_segmentations, part_id, num_parts, tta, fp16=fp16,\n",
        "                        overwrite_existing=overwrite, mode=mode, overwrite_all_in_gpu=all_in_gpu, step_size=step_size,\n",
        "                        force_separate_z=force_separate_z, interp_order=interp_order, interp_order_z=interp_order_z)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50ylZTrAuiCg",
        "colab_type": "text"
      },
      "source": [
        "The reconstruction process from the patch-segmentations back to the complete segmentation stretches across multiple classes in the nnUNet. The actual class discrimination in the nnUNet is done after the reconstruction is completed in the class segmentation_export.py in `/content/nnUNet/nnunet/inference`. But because attention maps generated by M3d-CAM are class discriminant by default (except for Guided Backpropagation) the nnUNet class discriminations needs to be disabled. This is done by commenting out the following block in segmentation_export.py and can be done by executing the next cell:\n",
        "\n",
        "```\n",
        "# Line 114-158\n",
        "# if region_class_order is None:\n",
        "#     seg_old_spacing = seg_old_spacing.argmax(0)\n",
        "# else:\n",
        "#     seg_old_spacing_final = np.zeros(seg_old_spacing.shape[1:])\n",
        "#     for i, c in enumerate(region_class_order):\n",
        "#         seg_old_spacing_final[seg_old_spacing[i] > 0.5] = c\n",
        "#     seg_old_spacing = seg_old_spacing_final\n",
        "seg_old_spacing = seg_old_spacing[0] * 255\n",
        "\n",
        "# Original nnUNet code ...\n",
        "# ...\n",
        "# ...\n",
        "\n",
        "return out_fname\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRKUNRpig7bR",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Modified segmentation_export.py - Replaces original segmentation_export.py\n",
        "%%writefile /content/nnUNet/nnunet/inference/segmentation_export.py\n",
        "\n",
        "#    Copyright 2020 Division of Medical Image Computing, German Cancer Research Center (DKFZ), Heidelberg, Germany\n",
        "#\n",
        "#    Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "#    you may not use this file except in compliance with the License.\n",
        "#    You may obtain a copy of the License at\n",
        "#\n",
        "#        http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "#    Unless required by applicable law or agreed to in writing, software\n",
        "#    distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "#    See the License for the specific language governing permissions and\n",
        "#    limitations under the License.\n",
        "\n",
        "\n",
        "import sys\n",
        "from copy import deepcopy\n",
        "from typing import Union, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import SimpleITK as sitk\n",
        "from batchgenerators.augmentations.utils import resize_segmentation\n",
        "from nnunet.preprocessing.preprocessing import get_lowres_axis, get_do_separate_z, resample_data_or_seg\n",
        "from batchgenerators.utilities.file_and_folder_operations import *\n",
        "\n",
        "\n",
        "def save_segmentation_nifti_from_softmax(segmentation_softmax: Union[str, np.ndarray], out_fname: str,\n",
        "                                         properties_dict: dict, order: int = 1,\n",
        "                                         region_class_order: Tuple[Tuple[int]] = None,\n",
        "                                         seg_postprogess_fn: callable = None, seg_postprocess_args: tuple = None,\n",
        "                                         resampled_npz_fname: str = None,\n",
        "                                         non_postprocessed_fname: str = None, force_separate_z: bool = None,\n",
        "                                         interpolation_order_z: int = 0, verbose: bool = True):\n",
        "    \"\"\"\n",
        "    This is a utility for writing segmentations to nifto and npz. It requires the data to have been preprocessed by\n",
        "    GenericPreprocessor because it depends on the property dictionary output (dct) to know the geometry of the original\n",
        "    data. segmentation_softmax does not have to have the same size in pixels as the original data, it will be\n",
        "    resampled to match that. This is generally useful because the spacings our networks operate on are most of the time\n",
        "    not the native spacings of the image data.\n",
        "    If seg_postprogess_fn is not None then seg_postprogess_fnseg_postprogess_fn(segmentation, *seg_postprocess_args)\n",
        "    will be called before nifto export\n",
        "    There is a problem with python process communication that prevents us from communicating obejcts\n",
        "    larger than 2 GB between processes (basically when the length of the pickle string that will be sent is\n",
        "    communicated by the multiprocessing.Pipe object then the placeholder (\\%i I think) does not allow for long\n",
        "    enough strings (lol). This could be fixed by changing i to l (for long) but that would require manually\n",
        "    patching system python code.) We circumvent that problem here by saving softmax_pred to a npy file that will\n",
        "    then be read (and finally deleted) by the Process. save_segmentation_nifti_from_softmax can take either\n",
        "    filename or np.ndarray for segmentation_softmax and will handle this automatically\n",
        "    :param segmentation_softmax:\n",
        "    :param out_fname:\n",
        "    :param properties_dict:\n",
        "    :param order:\n",
        "    :param region_class_order:\n",
        "    :param seg_postprogess_fn:\n",
        "    :param seg_postprocess_args:\n",
        "    :param resampled_npz_fname:\n",
        "    :param non_postprocessed_fname:\n",
        "    :param force_separate_z: if None then we dynamically decide how to resample along z, if True/False then always\n",
        "    /never resample along z separately. Do not touch unless you know what you are doing\n",
        "    :param interpolation_order_z: if separate z resampling is done then this is the order for resampling in z\n",
        "    :param verbose:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    if verbose: print(\"force_separate_z:\", force_separate_z, \"interpolation order:\", order)\n",
        "\n",
        "    if isinstance(segmentation_softmax, str):\n",
        "        assert isfile(segmentation_softmax), \"If isinstance(segmentation_softmax, str) then \" \\\n",
        "                                             \"isfile(segmentation_softmax) must be True\"\n",
        "        del_file = deepcopy(segmentation_softmax)\n",
        "        segmentation_softmax = np.load(segmentation_softmax)\n",
        "        os.remove(del_file)\n",
        "\n",
        "    # first resample, then put result into bbox of cropping, then save\n",
        "    current_shape = segmentation_softmax.shape\n",
        "    shape_original_after_cropping = properties_dict.get('size_after_cropping')\n",
        "    shape_original_before_cropping = properties_dict.get('original_size_of_raw_data')\n",
        "    # current_spacing = dct.get('spacing_after_resampling')\n",
        "    # original_spacing = dct.get('original_spacing')\n",
        "\n",
        "    if np.any([i != j for i, j in zip(np.array(current_shape[1:]), np.array(shape_original_after_cropping))]):\n",
        "        if force_separate_z is None:\n",
        "            if get_do_separate_z(properties_dict.get('original_spacing')):\n",
        "                do_separate_z = True\n",
        "                lowres_axis = get_lowres_axis(properties_dict.get('original_spacing'))\n",
        "            elif get_do_separate_z(properties_dict.get('spacing_after_resampling')):\n",
        "                do_separate_z = True\n",
        "                lowres_axis = get_lowres_axis(properties_dict.get('spacing_after_resampling'))\n",
        "            else:\n",
        "                do_separate_z = False\n",
        "                lowres_axis = None\n",
        "        else:\n",
        "            do_separate_z = force_separate_z\n",
        "            if do_separate_z:\n",
        "                lowres_axis = get_lowres_axis(properties_dict.get('original_spacing'))\n",
        "            else:\n",
        "                lowres_axis = None\n",
        "\n",
        "        if verbose: print(\"separate z:\", do_separate_z, \"lowres axis\", lowres_axis)\n",
        "        seg_old_spacing = resample_data_or_seg(segmentation_softmax, shape_original_after_cropping, is_seg=False,\n",
        "                                               axis=lowres_axis, order=order, do_separate_z=do_separate_z, cval=0,\n",
        "                                               order_z=interpolation_order_z)\n",
        "        # seg_old_spacing = resize_softmax_output(segmentation_softmax, shape_original_after_cropping, order=order)\n",
        "    else:\n",
        "        if verbose: print(\"no resampling necessary\")\n",
        "        seg_old_spacing = segmentation_softmax\n",
        "\n",
        "    if resampled_npz_fname is not None:\n",
        "        np.savez_compressed(resampled_npz_fname, softmax=seg_old_spacing.astype(np.float16))\n",
        "        save_pickle(properties_dict, resampled_npz_fname[:-4] + \".pkl\")\n",
        "\n",
        "    # if region_class_order is None:\n",
        "    #     seg_old_spacing = seg_old_spacing.argmax(0)\n",
        "    # else:\n",
        "    #     seg_old_spacing_final = np.zeros(seg_old_spacing.shape[1:])\n",
        "    #     for i, c in enumerate(region_class_order):\n",
        "    #         seg_old_spacing_final[seg_old_spacing[i] > 0.5] = c\n",
        "    #     seg_old_spacing = seg_old_spacing_final\n",
        "    seg_old_spacing = seg_old_spacing[0] * 255\n",
        "\n",
        "    bbox = properties_dict.get('crop_bbox')\n",
        "\n",
        "    if bbox is not None:\n",
        "        seg_old_size = np.zeros(shape_original_before_cropping)\n",
        "        for c in range(3):\n",
        "            bbox[c][1] = np.min((bbox[c][0] + seg_old_spacing.shape[c], shape_original_before_cropping[c]))\n",
        "        seg_old_size[bbox[0][0]:bbox[0][1],\n",
        "        bbox[1][0]:bbox[1][1],\n",
        "        bbox[2][0]:bbox[2][1]] = seg_old_spacing\n",
        "    else:\n",
        "        seg_old_size = seg_old_spacing\n",
        "\n",
        "    if seg_postprogess_fn is not None:\n",
        "        seg_old_size_postprocessed = seg_postprogess_fn(np.copy(seg_old_size), *seg_postprocess_args)\n",
        "    else:\n",
        "        seg_old_size_postprocessed = seg_old_size\n",
        "\n",
        "    seg_resized_itk = sitk.GetImageFromArray(seg_old_size_postprocessed.astype(np.uint8))\n",
        "    seg_resized_itk.SetSpacing(properties_dict['itk_spacing'])\n",
        "    seg_resized_itk.SetOrigin(properties_dict['itk_origin'])\n",
        "    seg_resized_itk.SetDirection(properties_dict['itk_direction'])\n",
        "    sitk.WriteImage(seg_resized_itk, out_fname)\n",
        "\n",
        "    if (non_postprocessed_fname is not None) and (seg_postprogess_fn is not None):\n",
        "        seg_resized_itk = sitk.GetImageFromArray(seg_old_size.astype(np.uint8))\n",
        "        seg_resized_itk.SetSpacing(properties_dict['itk_spacing'])\n",
        "        seg_resized_itk.SetOrigin(properties_dict['itk_origin'])\n",
        "        seg_resized_itk.SetDirection(properties_dict['itk_direction'])\n",
        "        sitk.WriteImage(seg_resized_itk, non_postprocessed_fname)\n",
        "          \n",
        "    return out_fname\n",
        "\n",
        "\n",
        "def save_segmentation_nifti(segmentation, out_fname, dct, order=1, force_separate_z=None):\n",
        "    \"\"\"\n",
        "    faster and uses less ram than save_segmentation_nifti_from_softmax, but maybe less precise and also does not support\n",
        "    softmax export (which is needed for ensembling). So it's a niche function that may be useful in some cases.\n",
        "    :param segmentation:\n",
        "    :param out_fname:\n",
        "    :param dct:\n",
        "    :param order:\n",
        "    :param force_separate_z:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    # suppress output\n",
        "    print(\"force_separate_z:\", force_separate_z, \"interpolation order:\", order)\n",
        "    sys.stdout = open(os.devnull, 'w')\n",
        "\n",
        "    if isinstance(segmentation, str):\n",
        "        assert isfile(segmentation), \"If isinstance(segmentation_softmax, str) then \" \\\n",
        "                                     \"isfile(segmentation_softmax) must be True\"\n",
        "        del_file = deepcopy(segmentation)\n",
        "        segmentation = np.load(segmentation)\n",
        "        os.remove(del_file)\n",
        "\n",
        "    # first resample, then put result into bbox of cropping, then save\n",
        "    current_shape = segmentation.shape\n",
        "    shape_original_after_cropping = dct.get('size_after_cropping')\n",
        "    shape_original_before_cropping = dct.get('original_size_of_raw_data')\n",
        "    # current_spacing = dct.get('spacing_after_resampling')\n",
        "    # original_spacing = dct.get('original_spacing')\n",
        "\n",
        "    if np.any(np.array(current_shape) != np.array(shape_original_after_cropping)):\n",
        "        if order == 0:\n",
        "            seg_old_spacing = resize_segmentation(segmentation, shape_original_after_cropping, 0, 0)\n",
        "        else:\n",
        "            if force_separate_z is None:\n",
        "                if get_do_separate_z(dct.get('original_spacing')):\n",
        "                    do_separate_z = True\n",
        "                    lowres_axis = get_lowres_axis(dct.get('original_spacing'))\n",
        "                elif get_do_separate_z(dct.get('spacing_after_resampling')):\n",
        "                    do_separate_z = True\n",
        "                    lowres_axis = get_lowres_axis(dct.get('spacing_after_resampling'))\n",
        "                else:\n",
        "                    do_separate_z = False\n",
        "                    lowres_axis = None\n",
        "            else:\n",
        "                do_separate_z = force_separate_z\n",
        "                if do_separate_z:\n",
        "                    lowres_axis = get_lowres_axis(dct.get('original_spacing'))\n",
        "                else:\n",
        "                    lowres_axis = None\n",
        "\n",
        "            print(\"separate z:\", do_separate_z, \"lowres axis\", lowres_axis)\n",
        "            seg_old_spacing = resample_data_or_seg(segmentation[None], shape_original_after_cropping, is_seg=True,\n",
        "                                                   axis=lowres_axis, order=order, do_separate_z=do_separate_z, cval=0)[\n",
        "                0]\n",
        "    else:\n",
        "        seg_old_spacing = segmentation\n",
        "\n",
        "    bbox = dct.get('crop_bbox')\n",
        "\n",
        "    if bbox is not None:\n",
        "        seg_old_size = np.zeros(shape_original_before_cropping)\n",
        "        for c in range(3):\n",
        "            bbox[c][1] = np.min((bbox[c][0] + seg_old_spacing.shape[c], shape_original_before_cropping[c]))\n",
        "        seg_old_size[bbox[0][0]:bbox[0][1],\n",
        "        bbox[1][0]:bbox[1][1],\n",
        "        bbox[2][0]:bbox[2][1]] = seg_old_spacing\n",
        "    else:\n",
        "        seg_old_size = seg_old_spacing\n",
        "\n",
        "    seg_resized_itk = sitk.GetImageFromArray(seg_old_size.astype(np.uint8))\n",
        "    seg_resized_itk.SetSpacing(dct['itk_spacing'])\n",
        "    seg_resized_itk.SetOrigin(dct['itk_origin'])\n",
        "    seg_resized_itk.SetDirection(dct['itk_direction'])\n",
        "    sitk.WriteImage(seg_resized_itk, out_fname)\n",
        "\n",
        "    sys.stdout = sys.__stdout__\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEBELq-JuQzv",
        "colab_type": "text"
      },
      "source": [
        "## Run inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wziBLg8FWoTP",
        "colab_type": "text"
      },
      "source": [
        "Now that everything is prepared you can run the actual inferece on some data. The generated attention maps are saved under  `/content/inference_results`. It might take some time for the first results to appear.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8VNvJKhYGVO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir /content/inference_results\n",
        "!nnUNet_predict -i /content/nnUNet_raw_data_base/nnUNet_raw_data/nnUNet_raw_data/Task005_Prostate/imagesTs -o /content/inference_results -t 5 -m 3d_fullres"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AumHWIukS0NT",
        "colab_type": "text"
      },
      "source": [
        "Now you can download the attention maps and inspect them (e.g. with ITK-Snap) if you want."
      ]
    }
  ]
}