<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.8.1" />
<title>medcam.medcam_inject API documentation</title>
<meta name="description" content="" />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>medcam.medcam_inject</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import torch
from pathlib import Path
import types
import pickle
from medcam import medcam_utils
from medcam.backends.guided_backpropagation import GuidedBackPropagation
from medcam.backends.grad_cam import GradCAM
from medcam.backends.guided_grad_cam import GuidedGradCam
from medcam.backends.grad_cam_pp import GradCamPP
from collections import defaultdict
from medcam.evaluation.evaluator import Evaluator
import copy
import numpy as np

def inject(model, output_dir=None, backend=&#39;medcam&#39;, layer=&#39;auto&#39;, label=None, data_shape=&#39;default&#39;,
           save_maps=False, save_pickle=False, save_scores=False, evaluate=False, metric=&#39;wioa&#39;, threshold=&#39;otsu&#39;, retain_graph=False,
           return_score=False, replace=False, cudnn=True, enabled=True):
    &#34;&#34;&#34;
    Injects a model with medcam functionality to extract attention maps from it. The model can be used as usual.
    Whenever model(input) or model.forward(input) is called medcam will extract the corresponding attention maps.
    Args:
        model: A CNN-based model that inherits from torch.nn.Module
        output_dir: The directory to save any results to
        backend: One of the implemented visualization backends.

                &#39;gbp&#39;: Guided-Backpropagation

                &#39;medcam&#39;: Grad-Cam

                &#39;ggcam&#39;: Guided-Grad-Cam

                &#39;gcampp&#39;: Grad-Cam++

        layer: One or multiple layer names of the model from which attention maps will be extracted.

                &#39;auto&#39;: Selects the last layer from which attention maps can be extracted.

                &#39;full&#39;: Selects every layer from which attention maps can be extracted.

                (layer name): A layer name of the model as string.

                [(layer name 1), (layer name 2), ...]: A list of layer names of the model as string.

            Note: Guided-Backpropagation ignores this parameter.

        label: A class label of interest. Alternatively this can be a class discriminator that creates a mask with only the non masked logits being backwarded through the model.

                Example for class label: label=1
                Example for discriminator: label=lambda x: 0.5 &lt; x

        data_shape: The shape of the resulting attention maps. The given shape should exclude batch and channel dimension.

                &#39;default&#39;: The shape of the current input data, excluding batch and channel dimension.

        save_maps: If the attention maps should be saved sorted by layer in the output_dir.

        save_pickle: If the attention maps should be saved as a pickle file in the output_dir.

        save_scores: If the evaluation scores should be saved as an excel file in the output_dir.

        evaluate: If the attention maps should be evaluated. This requires a corresponding mask when calling model.forward().

        metric: An evaluation metric for comparing the attention map with the mask.

                &#39;wioa&#39;: Weighted intersection over attention. Most suited for classification.

                &#39;ioa&#39;: Intersection over attention.

                &#39;iou&#39;: Intersection over union. Not suited for classification.

                (A function): An evaluation function.

        threshold: A threshold used during evaluation for ignoring low attention. Most models have low amounts of attention everywhere in an attention map due to the nature of CNN-based models. The threshold can be used to ignore these low amounts if wanted.

                &#39;otsu&#39;: Uses the otsu algorithm to determine a threshold.

                (float): A value between 0 and 1 that is used as threshold.

        retain_graph: If the computation graph should be retained or not.

        return_score: If the evaluation evaluation of the current input should be returned in addition to the model output.

        replace: If the model output should be replaced with the extracted attention map.

        cudnn: If cudnn should be disabled. Some models (e.g. LSTMs) crash when using medcam with enabled cudnn.

        enabled: If medcam should be enabled.

    Returns: A shallow copy of the model injected with medcam functionality.

    &#34;&#34;&#34;

    if _already_injected(model):
        return

    if not cudnn:
        torch.backends.cudnn.enabled = False

    if output_dir is not None:
        Path(output_dir).mkdir(parents=True, exist_ok=True)

    model_clone = copy.copy(model)
    model_clone.eval()
    # Save the original forward of the model
    # This forward will be called by the backend, so if someone writes a new backend they only need to call model.model_forward and not model.medcam_dict[&#39;model_forward&#39;]
    setattr(model_clone, &#39;model_forward&#39;, model_clone.forward)

    # Save every other attribute in a dict which is added to the model attributes
    # It is ugly but it avoids name conflicts
    medcam_dict = {}

    medcam_dict[&#39;output_dir&#39;] = output_dir
    medcam_dict[&#39;layer&#39;] = layer
    medcam_dict[&#39;counter&#39;] = 0
    medcam_dict[&#39;save_scores&#39;] = save_scores
    medcam_dict[&#39;save_maps&#39;] = save_maps
    medcam_dict[&#39;save_pickle&#39;] = save_pickle
    medcam_dict[&#39;evaluate&#39;] = evaluate
    medcam_dict[&#39;metric&#39;] = metric
    medcam_dict[&#39;return_score&#39;] = return_score
    medcam_dict[&#39;_replace_output&#39;] = replace
    medcam_dict[&#39;threshold&#39;] = threshold
    medcam_dict[&#39;label&#39;] = label
    medcam_dict[&#39;channels&#39;] = 1  # TODO: Remove in a later version
    medcam_dict[&#39;data_shape&#39;] = data_shape
    medcam_dict[&#39;pickle_maps&#39;] = []
    if evaluate:
        medcam_dict[&#39;Evaluator&#39;] = Evaluator(output_dir + &#34;/&#34;, metric=metric, threshold=threshold, layer_ordering=medcam_utils.get_layers(model_clone))
    medcam_dict[&#39;current_attention_map&#39;] = None
    medcam_dict[&#39;current_layer&#39;] = None
    medcam_dict[&#39;device&#39;] = next(model_clone.parameters()).device
    medcam_dict[&#39;tested&#39;] = False
    medcam_dict[&#39;enabled&#39;] = enabled
    setattr(model_clone, &#39;medcam_dict&#39;, medcam_dict)

    if output_dir is None and (save_scores is not None or save_maps is not None or save_pickle is not None or evaluate):
        raise ValueError(&#34;output_dir needs to be set if save_scores, save_maps, save_pickle or evaluate is set to true&#34;)

    # Append methods methods to the model
    model_clone.get_layers = types.MethodType(get_layers, model_clone)
    model_clone.get_attention_map = types.MethodType(get_attention_map, model_clone)
    model_clone.save_attention_map = types.MethodType(save_attention_map, model_clone)
    model_clone.replace_output = types.MethodType(replace_output, model_clone)
    model_clone.dump = types.MethodType(dump, model_clone)
    model_clone.forward = types.MethodType(forward, model_clone)
    model_clone.enable_medcam = types.MethodType(enable_medcam, model_clone)
    model_clone.disable_medcam = types.MethodType(disable_medcam, model_clone)
    model_clone.test_run = types.MethodType(test_run, model_clone)

    model_clone._assign_backend = types.MethodType(_assign_backend, model_clone)
    model_clone._process_attention_maps = types.MethodType(_process_attention_maps, model_clone)
    model_clone._save_attention_map = types.MethodType(_save_attention_map, model_clone)
    model_clone._replace_output = types.MethodType(_replace_output, model_clone)

    model_backend, heatmap = _assign_backend(backend, model_clone, layer, None, retain_graph)  # TODO: Remove postprocessor in a later version
    medcam_dict[&#39;model_backend&#39;] = model_backend
    medcam_dict[&#39;heatmap&#39;] = heatmap

    return model_clone

def get_layers(self, reverse=False):
    &#34;&#34;&#34;Returns the layers of the model. Optionally reverses the order of the layers.&#34;&#34;&#34;
    return self.medcam_dict[&#39;model_backend&#39;].layers(reverse)

def get_attention_map(self):
    &#34;&#34;&#34;Returns the current attention map.&#34;&#34;&#34;
    return self.medcam_dict[&#39;current_attention_map&#39;]

def save_attention_map(self, attention_map):
    &#34;&#34;&#34;Saves an attention map.&#34;&#34;&#34;
    medcam_utils.save_attention_map(filename=self.medcam_dict[&#39;output_dir&#39;] + &#34;/&#34; + self.medcam_dict[&#39;current_layer&#39;] + &#34;/attention_map_&#34; +
                                             str(self.medcam_dict[&#39;counter&#39;]), attention_map=attention_map, heatmap=self.medcam_dict[&#39;heatmap&#39;])
    self.medcam_dict[&#39;counter&#39;] += 1

def replace_output(self, replace):
    &#34;&#34;&#34;If the output should be replaced with the corresponiding attention map.&#34;&#34;&#34;
    self.medcam_dict[&#39;_replace_output&#39;] = replace

def dump(self):
    &#34;&#34;&#34;Saves all of the collected data to the output directory.&#34;&#34;&#34;
    if self.medcam_dict[&#39;save_pickle&#39;]:
        with open(self.medcam_dict[&#39;output_dir&#39;] + &#39;/attention_maps.pkl&#39;, &#39;wb&#39;) as handle:  # TODO: Save every 1GB
            pickle.dump(self.medcam_dict[&#39;pickle_maps&#39;], handle, protocol=pickle.HIGHEST_PROTOCOL)
    if self.medcam_dict[&#39;save_scores&#39;]:
        self.medcam_dict[&#39;Evaluator&#39;].dump()

def forward(self, batch, label=None, mask=None, raw_input=None):
    &#34;&#34;&#34;
    Generates attention maps for a given batch input.
    Args:
        batch: An input batch of shape (BxCxHxW) or (BxCxDxHxW).
        label: A class label (int) or a class discriminator function to different attention maps for every class.
        mask: A ground truth mask corresponding to the input batch. Only needed when evaluate is set to true.

    Returns: Either the normal output of the model or an attention map.

    &#34;&#34;&#34;
    if self.medcam_dict[&#39;enabled&#39;]:
        self.test_run(batch, internal=True)
        if self.medcam_dict[&#39;layer&#39;] == &#39;full&#39; and not self.medcam_dict[&#39;tested&#39;]:
            raise ValueError(&#34;Layer mode &#39;full&#39; requires a test run either during injection or by calling test_run() afterwards&#34;)
        with torch.enable_grad():
            output, attention_map, batch_size, channels, data_shape = self.medcam_dict[&#39;model_backend&#39;].generate_attention_map(batch, label)
            if attention_map:
                if len(attention_map.keys()) == 1:
                    self.medcam_dict[&#39;current_attention_map&#39;] = attention_map[list(attention_map.keys())[0]]
                    self.medcam_dict[&#39;current_layer&#39;] = list(attention_map.keys())[0]
                scores = self._process_attention_maps(attention_map, mask, batch_size, channels, raw_input)
                output = self._replace_output(output, attention_map, data_shape)
            else:  # If no attention maps could be extracted
                self.medcam_dict[&#39;current_attention_map&#39;] = None
                self.medcam_dict[&#39;current_layer&#39;] = None
                scores = None
                if self.medcam_dict[&#39;_replace_output&#39;]:
                    raise ValueError(&#34;Unable to extract any attention maps&#34;)
            self.medcam_dict[&#39;counter&#39;] += 1
            if self.medcam_dict[&#39;return_score&#39;]:
                return output, scores
            else:
                return output
    else:
        return self.model_forward(batch)

def test_run(self, batch, internal=False):
    &#34;&#34;&#34;Performs a test run. This allows medcam to determine for which layers it can generate attention maps.&#34;&#34;&#34;
    registered_hooks = []
    if batch is not None and not self.medcam_dict[&#39;tested&#39;]:
        with torch.enable_grad():
            _ = self.medcam_dict[&#39;model_backend&#39;].generate_attention_map(batch, None)
            registered_hooks = self.medcam_dict[&#39;model_backend&#39;].get_registered_hooks()
        self.medcam_dict[&#39;tested&#39;] = True
        if not internal:
            print(&#34;Successfully registered to the following layers: &#34;, registered_hooks)
            if self.medcam_dict[&#39;output_dir&#39;] is not None:
                np.savetxt(self.medcam_dict[&#39;output_dir&#39;] + &#39;/registered_layers.txt&#39;, np.asarray(registered_hooks).astype(str), fmt=&#34;%s&#34;)
    return registered_hooks

def disable_medcam(self):
    &#34;&#34;&#34;Disables medcam.&#34;&#34;&#34;
    self.medcam_dict[&#39;enabled&#39;] = False

def enable_medcam(self):
    &#34;&#34;&#34;Enables medcam.&#34;&#34;&#34;
    self.medcam_dict[&#39;enabled&#39;] = True

def _already_injected(model):
    &#34;&#34;&#34;Checks if the model is already injected with medcam.&#34;&#34;&#34;
    try:  # try/except is faster than hasattr, if inject method is called repeatedly
        model.medcam_dict  # Check if attribute exists
        return True
    except AttributeError:
        return False

def _assign_backend(backend, model, target_layers, postprocessor, retain_graph):
    &#34;&#34;&#34;Assigns a chosen backend.&#34;&#34;&#34;
    if backend == &#34;gbp&#34;:
        return GuidedBackPropagation(model=model, postprocessor=postprocessor, retain_graph=retain_graph), False
    elif backend == &#34;medcam&#34;:
        return GradCAM(model=model, target_layers=target_layers, postprocessor=postprocessor, retain_graph=retain_graph), True
    elif backend == &#34;ggcam&#34;:
        return GuidedGradCam(model=model, target_layers=target_layers, postprocessor=postprocessor, retain_graph=retain_graph), False
    elif backend == &#34;gcampp&#34;:
        return GradCamPP(model=model, target_layers=target_layers, postprocessor=postprocessor, retain_graph=retain_graph), True
    else:
        raise ValueError(&#34;Backend does not exist&#34;)

def _process_attention_maps(self, attention_map, mask, batch_size, channels, raw_input):
    &#34;&#34;&#34;Handles all the stuff after the attention map has been generated. Like creating dictionaries, saving the attention map and doing the evaluation.&#34;&#34;&#34;
    batch_scores = defaultdict(list) if self.medcam_dict[&#39;evaluate&#39;] else None
    raw_input_single = None
    for layer_name in attention_map.keys():
        layer_output_dir = None
        if self.medcam_dict[&#39;output_dir&#39;] is not None and self.medcam_dict[&#39;save_maps&#39;]:
            if layer_name == &#34;&#34;:
                layer_output_dir = self.medcam_dict[&#39;output_dir&#39;]
            else:
                layer_output_dir = self.medcam_dict[&#39;output_dir&#39;] + &#34;/&#34; + layer_name
            Path(layer_output_dir).mkdir(parents=True, exist_ok=True)
        for j in range(batch_size):
            for k in range(channels):
                attention_map_single = attention_map[layer_name][j][k]
                if raw_input is not None:
                    raw_input_single = raw_input[j]
                self._save_attention_map(attention_map_single, layer_output_dir, j, k, raw_input_single)
                if self.medcam_dict[&#39;evaluate&#39;]:
                    if mask is None:
                        raise ValueError(&#34;Mask cannot be none in evaluation mode&#34;)
                    score = self.medcam_dict[&#39;Evaluator&#39;].comp_score(attention_map_single, mask[j][k].squeeze(), layer=layer_name, class_label=k)
                    batch_scores[layer_name].append(score)
    return batch_scores

def _save_attention_map(self, attention_map, layer_output_dir, j, k, raw_input):
    &#34;&#34;&#34;Internal method for saving saving an attention map.&#34;&#34;&#34;
    if self.medcam_dict[&#39;save_pickle&#39;]:
        self.medcam_dict[&#39;pickle_maps&#39;].append(attention_map)
    if self.medcam_dict[&#39;save_maps&#39;]:
        medcam_utils.save_attention_map(filename=layer_output_dir + &#34;/attention_map_&#34; + str(self.medcam_dict[&#39;counter&#39;]) + &#34;_&#34; + str(j) + &#34;_&#34; + str(k), attention_map=attention_map, heatmap=self.medcam_dict[&#39;heatmap&#39;], raw_input=raw_input)

def _replace_output(self, output, attention_map, data_shape):
    &#34;&#34;&#34;Replaces the model output with the current attention map.&#34;&#34;&#34;
    if self.medcam_dict[&#39;_replace_output&#39;]:
        if len(attention_map.keys()) == 1:
            output = torch.tensor(self.medcam_dict[&#39;current_attention_map&#39;]).to(str(self.medcam_dict[&#39;device&#39;]))
            if data_shape is not None:  # If data_shape is None then the task is classification -&gt; return unchanged attention map
                output = medcam_utils.interpolate(output, data_shape)
        else:
            raise ValueError(&#34;Not possible to replace output when layer is &#39;full&#39;, only with &#39;auto&#39; or a manually set layer&#34;)
    return output</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="medcam.medcam_inject.disable_medcam"><code class="name flex">
<span>def <span class="ident">disable_medcam</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Disables medcam.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def disable_medcam(self):
    &#34;&#34;&#34;Disables medcam.&#34;&#34;&#34;
    self.medcam_dict[&#39;enabled&#39;] = False</code></pre>
</details>
</dd>
<dt id="medcam.medcam_inject.dump"><code class="name flex">
<span>def <span class="ident">dump</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Saves all of the collected data to the output directory.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def dump(self):
    &#34;&#34;&#34;Saves all of the collected data to the output directory.&#34;&#34;&#34;
    if self.medcam_dict[&#39;save_pickle&#39;]:
        with open(self.medcam_dict[&#39;output_dir&#39;] + &#39;/attention_maps.pkl&#39;, &#39;wb&#39;) as handle:  # TODO: Save every 1GB
            pickle.dump(self.medcam_dict[&#39;pickle_maps&#39;], handle, protocol=pickle.HIGHEST_PROTOCOL)
    if self.medcam_dict[&#39;save_scores&#39;]:
        self.medcam_dict[&#39;Evaluator&#39;].dump()</code></pre>
</details>
</dd>
<dt id="medcam.medcam_inject.enable_medcam"><code class="name flex">
<span>def <span class="ident">enable_medcam</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Enables medcam.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def enable_medcam(self):
    &#34;&#34;&#34;Enables medcam.&#34;&#34;&#34;
    self.medcam_dict[&#39;enabled&#39;] = True</code></pre>
</details>
</dd>
<dt id="medcam.medcam_inject.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, batch, label=None, mask=None, raw_input=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Generates attention maps for a given batch input.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>batch</code></strong></dt>
<dd>An input batch of shape (BxCxHxW) or (BxCxDxHxW).</dd>
<dt><strong><code>label</code></strong></dt>
<dd>A class label (int) or a class discriminator function to different attention maps for every class.</dd>
<dt><strong><code>mask</code></strong></dt>
<dd>A ground truth mask corresponding to the input batch. Only needed when evaluate is set to true.</dd>
</dl>
<p>Returns: Either the normal output of the model or an attention map.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, batch, label=None, mask=None, raw_input=None):
    &#34;&#34;&#34;
    Generates attention maps for a given batch input.
    Args:
        batch: An input batch of shape (BxCxHxW) or (BxCxDxHxW).
        label: A class label (int) or a class discriminator function to different attention maps for every class.
        mask: A ground truth mask corresponding to the input batch. Only needed when evaluate is set to true.

    Returns: Either the normal output of the model or an attention map.

    &#34;&#34;&#34;
    if self.medcam_dict[&#39;enabled&#39;]:
        self.test_run(batch, internal=True)
        if self.medcam_dict[&#39;layer&#39;] == &#39;full&#39; and not self.medcam_dict[&#39;tested&#39;]:
            raise ValueError(&#34;Layer mode &#39;full&#39; requires a test run either during injection or by calling test_run() afterwards&#34;)
        with torch.enable_grad():
            output, attention_map, batch_size, channels, data_shape = self.medcam_dict[&#39;model_backend&#39;].generate_attention_map(batch, label)
            if attention_map:
                if len(attention_map.keys()) == 1:
                    self.medcam_dict[&#39;current_attention_map&#39;] = attention_map[list(attention_map.keys())[0]]
                    self.medcam_dict[&#39;current_layer&#39;] = list(attention_map.keys())[0]
                scores = self._process_attention_maps(attention_map, mask, batch_size, channels, raw_input)
                output = self._replace_output(output, attention_map, data_shape)
            else:  # If no attention maps could be extracted
                self.medcam_dict[&#39;current_attention_map&#39;] = None
                self.medcam_dict[&#39;current_layer&#39;] = None
                scores = None
                if self.medcam_dict[&#39;_replace_output&#39;]:
                    raise ValueError(&#34;Unable to extract any attention maps&#34;)
            self.medcam_dict[&#39;counter&#39;] += 1
            if self.medcam_dict[&#39;return_score&#39;]:
                return output, scores
            else:
                return output
    else:
        return self.model_forward(batch)</code></pre>
</details>
</dd>
<dt id="medcam.medcam_inject.get_attention_map"><code class="name flex">
<span>def <span class="ident">get_attention_map</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the current attention map.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_attention_map(self):
    &#34;&#34;&#34;Returns the current attention map.&#34;&#34;&#34;
    return self.medcam_dict[&#39;current_attention_map&#39;]</code></pre>
</details>
</dd>
<dt id="medcam.medcam_inject.get_layers"><code class="name flex">
<span>def <span class="ident">get_layers</span></span>(<span>self, reverse=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the layers of the model. Optionally reverses the order of the layers.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_layers(self, reverse=False):
    &#34;&#34;&#34;Returns the layers of the model. Optionally reverses the order of the layers.&#34;&#34;&#34;
    return self.medcam_dict[&#39;model_backend&#39;].layers(reverse)</code></pre>
</details>
</dd>
<dt id="medcam.medcam_inject.inject"><code class="name flex">
<span>def <span class="ident">inject</span></span>(<span>model, output_dir=None, backend='medcam', layer='auto', label=None, data_shape='default', save_maps=False, save_pickle=False, save_scores=False, evaluate=False, metric='wioa', threshold='otsu', retain_graph=False, return_score=False, replace=False, cudnn=True, enabled=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Injects a model with medcam functionality to extract attention maps from it. The model can be used as usual.
Whenever model(input) or model.forward(input) is called medcam will extract the corresponding attention maps.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>model</code></strong></dt>
<dd>A CNN-based model that inherits from torch.nn.Module</dd>
<dt><strong><code>output_dir</code></strong></dt>
<dd>The directory to save any results to</dd>
<dt><strong><code>backend</code></strong></dt>
<dd>One of the implemented visualization backends.<pre><code>'gbp': Guided-Backpropagation

'medcam': Grad-Cam

'ggcam': Guided-Grad-Cam

'gcampp': Grad-Cam++
</code></pre>
</dd>
<dt><strong><code>layer</code></strong></dt>
<dd>
<p>One or multiple layer names of the model from which attention maps will be extracted.</p>
<pre><code>'auto': Selects the last layer from which attention maps can be extracted.

'full': Selects every layer from which attention maps can be extracted.

(layer name): A layer name of the model as string.

[(layer name 1), (layer name 2), ...]: A list of layer names of the model as string.
</code></pre>
<p>Note: Guided-Backpropagation ignores this parameter.</p>
</dd>
<dt><strong><code>label</code></strong></dt>
<dd>A class label of interest. Alternatively this can be a class discriminator that creates a mask with only the non masked logits being backwarded through the model.<pre><code>Example for class label: label=1
Example for discriminator: label=lambda x: 0.5 &lt; x
</code></pre>
</dd>
<dt><strong><code>data_shape</code></strong></dt>
<dd>The shape of the resulting attention maps. The given shape should exclude batch and channel dimension.<pre><code>'default': The shape of the current input data, excluding batch and channel dimension.
</code></pre>
</dd>
<dt><strong><code>save_maps</code></strong></dt>
<dd>If the attention maps should be saved sorted by layer in the output_dir.</dd>
<dt><strong><code>save_pickle</code></strong></dt>
<dd>If the attention maps should be saved as a pickle file in the output_dir.</dd>
<dt><strong><code>save_scores</code></strong></dt>
<dd>If the evaluation scores should be saved as an excel file in the output_dir.</dd>
<dt><strong><code>evaluate</code></strong></dt>
<dd>If the attention maps should be evaluated. This requires a corresponding mask when calling model.forward().</dd>
<dt><strong><code>metric</code></strong></dt>
<dd>An evaluation metric for comparing the attention map with the mask.<pre><code>'wioa': Weighted intersection over attention. Most suited for classification.

'ioa': Intersection over attention.

'iou': Intersection over union. Not suited for classification.

(A function): An evaluation function.
</code></pre>
</dd>
<dt><strong><code>threshold</code></strong></dt>
<dd>A threshold used during evaluation for ignoring low attention. Most models have low amounts of attention everywhere in an attention map due to the nature of CNN-based models. The threshold can be used to ignore these low amounts if wanted.<pre><code>'otsu': Uses the otsu algorithm to determine a threshold.

(float): A value between 0 and 1 that is used as threshold.
</code></pre>
</dd>
<dt><strong><code>retain_graph</code></strong></dt>
<dd>If the computation graph should be retained or not.</dd>
<dt><strong><code>return_score</code></strong></dt>
<dd>If the evaluation evaluation of the current input should be returned in addition to the model output.</dd>
<dt><strong><code>replace</code></strong></dt>
<dd>If the model output should be replaced with the extracted attention map.</dd>
<dt><strong><code>cudnn</code></strong></dt>
<dd>If cudnn should be disabled. Some models (e.g. LSTMs) crash when using medcam with enabled cudnn.</dd>
<dt><strong><code>enabled</code></strong></dt>
<dd>If medcam should be enabled.</dd>
</dl>
<p>Returns: A shallow copy of the model injected with medcam functionality.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def inject(model, output_dir=None, backend=&#39;medcam&#39;, layer=&#39;auto&#39;, label=None, data_shape=&#39;default&#39;,
           save_maps=False, save_pickle=False, save_scores=False, evaluate=False, metric=&#39;wioa&#39;, threshold=&#39;otsu&#39;, retain_graph=False,
           return_score=False, replace=False, cudnn=True, enabled=True):
    &#34;&#34;&#34;
    Injects a model with medcam functionality to extract attention maps from it. The model can be used as usual.
    Whenever model(input) or model.forward(input) is called medcam will extract the corresponding attention maps.
    Args:
        model: A CNN-based model that inherits from torch.nn.Module
        output_dir: The directory to save any results to
        backend: One of the implemented visualization backends.

                &#39;gbp&#39;: Guided-Backpropagation

                &#39;medcam&#39;: Grad-Cam

                &#39;ggcam&#39;: Guided-Grad-Cam

                &#39;gcampp&#39;: Grad-Cam++

        layer: One or multiple layer names of the model from which attention maps will be extracted.

                &#39;auto&#39;: Selects the last layer from which attention maps can be extracted.

                &#39;full&#39;: Selects every layer from which attention maps can be extracted.

                (layer name): A layer name of the model as string.

                [(layer name 1), (layer name 2), ...]: A list of layer names of the model as string.

            Note: Guided-Backpropagation ignores this parameter.

        label: A class label of interest. Alternatively this can be a class discriminator that creates a mask with only the non masked logits being backwarded through the model.

                Example for class label: label=1
                Example for discriminator: label=lambda x: 0.5 &lt; x

        data_shape: The shape of the resulting attention maps. The given shape should exclude batch and channel dimension.

                &#39;default&#39;: The shape of the current input data, excluding batch and channel dimension.

        save_maps: If the attention maps should be saved sorted by layer in the output_dir.

        save_pickle: If the attention maps should be saved as a pickle file in the output_dir.

        save_scores: If the evaluation scores should be saved as an excel file in the output_dir.

        evaluate: If the attention maps should be evaluated. This requires a corresponding mask when calling model.forward().

        metric: An evaluation metric for comparing the attention map with the mask.

                &#39;wioa&#39;: Weighted intersection over attention. Most suited for classification.

                &#39;ioa&#39;: Intersection over attention.

                &#39;iou&#39;: Intersection over union. Not suited for classification.

                (A function): An evaluation function.

        threshold: A threshold used during evaluation for ignoring low attention. Most models have low amounts of attention everywhere in an attention map due to the nature of CNN-based models. The threshold can be used to ignore these low amounts if wanted.

                &#39;otsu&#39;: Uses the otsu algorithm to determine a threshold.

                (float): A value between 0 and 1 that is used as threshold.

        retain_graph: If the computation graph should be retained or not.

        return_score: If the evaluation evaluation of the current input should be returned in addition to the model output.

        replace: If the model output should be replaced with the extracted attention map.

        cudnn: If cudnn should be disabled. Some models (e.g. LSTMs) crash when using medcam with enabled cudnn.

        enabled: If medcam should be enabled.

    Returns: A shallow copy of the model injected with medcam functionality.

    &#34;&#34;&#34;

    if _already_injected(model):
        return

    if not cudnn:
        torch.backends.cudnn.enabled = False

    if output_dir is not None:
        Path(output_dir).mkdir(parents=True, exist_ok=True)

    model_clone = copy.copy(model)
    model_clone.eval()
    # Save the original forward of the model
    # This forward will be called by the backend, so if someone writes a new backend they only need to call model.model_forward and not model.medcam_dict[&#39;model_forward&#39;]
    setattr(model_clone, &#39;model_forward&#39;, model_clone.forward)

    # Save every other attribute in a dict which is added to the model attributes
    # It is ugly but it avoids name conflicts
    medcam_dict = {}

    medcam_dict[&#39;output_dir&#39;] = output_dir
    medcam_dict[&#39;layer&#39;] = layer
    medcam_dict[&#39;counter&#39;] = 0
    medcam_dict[&#39;save_scores&#39;] = save_scores
    medcam_dict[&#39;save_maps&#39;] = save_maps
    medcam_dict[&#39;save_pickle&#39;] = save_pickle
    medcam_dict[&#39;evaluate&#39;] = evaluate
    medcam_dict[&#39;metric&#39;] = metric
    medcam_dict[&#39;return_score&#39;] = return_score
    medcam_dict[&#39;_replace_output&#39;] = replace
    medcam_dict[&#39;threshold&#39;] = threshold
    medcam_dict[&#39;label&#39;] = label
    medcam_dict[&#39;channels&#39;] = 1  # TODO: Remove in a later version
    medcam_dict[&#39;data_shape&#39;] = data_shape
    medcam_dict[&#39;pickle_maps&#39;] = []
    if evaluate:
        medcam_dict[&#39;Evaluator&#39;] = Evaluator(output_dir + &#34;/&#34;, metric=metric, threshold=threshold, layer_ordering=medcam_utils.get_layers(model_clone))
    medcam_dict[&#39;current_attention_map&#39;] = None
    medcam_dict[&#39;current_layer&#39;] = None
    medcam_dict[&#39;device&#39;] = next(model_clone.parameters()).device
    medcam_dict[&#39;tested&#39;] = False
    medcam_dict[&#39;enabled&#39;] = enabled
    setattr(model_clone, &#39;medcam_dict&#39;, medcam_dict)

    if output_dir is None and (save_scores is not None or save_maps is not None or save_pickle is not None or evaluate):
        raise ValueError(&#34;output_dir needs to be set if save_scores, save_maps, save_pickle or evaluate is set to true&#34;)

    # Append methods methods to the model
    model_clone.get_layers = types.MethodType(get_layers, model_clone)
    model_clone.get_attention_map = types.MethodType(get_attention_map, model_clone)
    model_clone.save_attention_map = types.MethodType(save_attention_map, model_clone)
    model_clone.replace_output = types.MethodType(replace_output, model_clone)
    model_clone.dump = types.MethodType(dump, model_clone)
    model_clone.forward = types.MethodType(forward, model_clone)
    model_clone.enable_medcam = types.MethodType(enable_medcam, model_clone)
    model_clone.disable_medcam = types.MethodType(disable_medcam, model_clone)
    model_clone.test_run = types.MethodType(test_run, model_clone)

    model_clone._assign_backend = types.MethodType(_assign_backend, model_clone)
    model_clone._process_attention_maps = types.MethodType(_process_attention_maps, model_clone)
    model_clone._save_attention_map = types.MethodType(_save_attention_map, model_clone)
    model_clone._replace_output = types.MethodType(_replace_output, model_clone)

    model_backend, heatmap = _assign_backend(backend, model_clone, layer, None, retain_graph)  # TODO: Remove postprocessor in a later version
    medcam_dict[&#39;model_backend&#39;] = model_backend
    medcam_dict[&#39;heatmap&#39;] = heatmap

    return model_clone</code></pre>
</details>
</dd>
<dt id="medcam.medcam_inject.replace_output"><code class="name flex">
<span>def <span class="ident">replace_output</span></span>(<span>self, replace)</span>
</code></dt>
<dd>
<div class="desc"><p>If the output should be replaced with the corresponiding attention map.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def replace_output(self, replace):
    &#34;&#34;&#34;If the output should be replaced with the corresponiding attention map.&#34;&#34;&#34;
    self.medcam_dict[&#39;_replace_output&#39;] = replace</code></pre>
</details>
</dd>
<dt id="medcam.medcam_inject.save_attention_map"><code class="name flex">
<span>def <span class="ident">save_attention_map</span></span>(<span>self, attention_map)</span>
</code></dt>
<dd>
<div class="desc"><p>Saves an attention map.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def save_attention_map(self, attention_map):
    &#34;&#34;&#34;Saves an attention map.&#34;&#34;&#34;
    medcam_utils.save_attention_map(filename=self.medcam_dict[&#39;output_dir&#39;] + &#34;/&#34; + self.medcam_dict[&#39;current_layer&#39;] + &#34;/attention_map_&#34; +
                                             str(self.medcam_dict[&#39;counter&#39;]), attention_map=attention_map, heatmap=self.medcam_dict[&#39;heatmap&#39;])
    self.medcam_dict[&#39;counter&#39;] += 1</code></pre>
</details>
</dd>
<dt id="medcam.medcam_inject.test_run"><code class="name flex">
<span>def <span class="ident">test_run</span></span>(<span>self, batch, internal=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Performs a test run. This allows medcam to determine for which layers it can generate attention maps.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_run(self, batch, internal=False):
    &#34;&#34;&#34;Performs a test run. This allows medcam to determine for which layers it can generate attention maps.&#34;&#34;&#34;
    registered_hooks = []
    if batch is not None and not self.medcam_dict[&#39;tested&#39;]:
        with torch.enable_grad():
            _ = self.medcam_dict[&#39;model_backend&#39;].generate_attention_map(batch, None)
            registered_hooks = self.medcam_dict[&#39;model_backend&#39;].get_registered_hooks()
        self.medcam_dict[&#39;tested&#39;] = True
        if not internal:
            print(&#34;Successfully registered to the following layers: &#34;, registered_hooks)
            if self.medcam_dict[&#39;output_dir&#39;] is not None:
                np.savetxt(self.medcam_dict[&#39;output_dir&#39;] + &#39;/registered_layers.txt&#39;, np.asarray(registered_hooks).astype(str), fmt=&#34;%s&#34;)
    return registered_hooks</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="medcam" href="index.html">medcam</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="two-column">
<li><code><a title="medcam.medcam_inject.disable_medcam" href="#medcam.medcam_inject.disable_medcam">disable_medcam</a></code></li>
<li><code><a title="medcam.medcam_inject.dump" href="#medcam.medcam_inject.dump">dump</a></code></li>
<li><code><a title="medcam.medcam_inject.enable_medcam" href="#medcam.medcam_inject.enable_medcam">enable_medcam</a></code></li>
<li><code><a title="medcam.medcam_inject.forward" href="#medcam.medcam_inject.forward">forward</a></code></li>
<li><code><a title="medcam.medcam_inject.get_attention_map" href="#medcam.medcam_inject.get_attention_map">get_attention_map</a></code></li>
<li><code><a title="medcam.medcam_inject.get_layers" href="#medcam.medcam_inject.get_layers">get_layers</a></code></li>
<li><code><a title="medcam.medcam_inject.inject" href="#medcam.medcam_inject.inject">inject</a></code></li>
<li><code><a title="medcam.medcam_inject.replace_output" href="#medcam.medcam_inject.replace_output">replace_output</a></code></li>
<li><code><a title="medcam.medcam_inject.save_attention_map" href="#medcam.medcam_inject.save_attention_map">save_attention_map</a></code></li>
<li><code><a title="medcam.medcam_inject.test_run" href="#medcam.medcam_inject.test_run">test_run</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.8.1</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>